{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "637e2bb2-69b9-4f8d-a25f-f29fa8c74419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# Identifying all PDFs\n",
    "pdf_folder = 'data/modules'\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "total_files = len(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8763adc4-5539-4593-8815-7f91d9449d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF using pdfplumber\n",
    "def extract_pdf_text(pdf_path):\n",
    "    all_text = ''\n",
    "    with contextlib.redirect_stderr(io.StringIO()): # Use of AI to avoid warning messages\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    all_text += page_text + '\\n'\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec76e4dd-ac90-463a-a0dc-8739252751af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract department name & code\n",
    "def extract_department_code(text):\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        match = re.search(r'([A-Za-z]+) \\(([A-Za-z]{2,4})\\) course results', line) # Use of AI to generate generic re code that identifies string\n",
    "        if match:\n",
    "            department_name = match.group(1) # Use of AI to learn about .group() function\n",
    "            department_code = match.group(2)\n",
    "            break\n",
    "    return department_code, department_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c181e6e5-36f9-4681-b714-8db93002730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Marksummary tables from text\n",
    "def extract_marksummary(text, department_code, department_name):\n",
    "    lines = text.split('\\n')\n",
    "    mark_data = []\n",
    "    excluded_data = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        # Identifying table headers\n",
    "        if line.startswith('Year marks mean sd'):\n",
    "            header = line.split()\n",
    "            pos = {col: idx for idx, col in enumerate(header)}\n",
    "\n",
    "            # Identifying course code (at bottom of table)\n",
    "            course = department_code # Setting department code as default\n",
    "            for k in range(1, 7):\n",
    "                if i + k < len(lines):\n",
    "                    match = re.search(r'([A-Z0-9]+):Marksummary', lines[i + k])\n",
    "                    if match:\n",
    "                        course = match.group(1)\n",
    "                        break\n",
    "\n",
    "            # Moving to first data row (skipping header)\n",
    "            i += 1\n",
    "            course_data = []\n",
    "            skipped_rows = []\n",
    "            \n",
    "            while i < len(lines):\n",
    "                line = lines[i].strip()\n",
    "                # Break when encountering table title (at bottom of each table)\n",
    "                if re.match(r'([A-Z0-9]+):Marksummary', line) or line.startswith('MarksbyYear'):\n",
    "                    break\n",
    "\n",
    "                # Parsing data\n",
    "                if line:\n",
    "                    values = line.split()\n",
    "                    if len(values) == len(pos): # cleaning data from incomplete and misaligned rows due to missing values\n",
    "                        course_data.append({\n",
    "                            'department': department_name,\n",
    "                            'code': course,\n",
    "                            'year': values[pos['Year']],\n",
    "                            'marks': int(values[pos['marks']]),\n",
    "                            'mean': float(values[pos['mean']]),\n",
    "                            'sd': float(values[pos['sd']]),\n",
    "                            'min': float(values[pos['min']]),\n",
    "                            'q10': float(values[pos['q10']]),\n",
    "                            'q25': float(values[pos['q25']]),\n",
    "                            'median': float(values[pos['median']]),\n",
    "                            'IQR': float(values[pos['IQR']]),\n",
    "                            'q75': float(values[pos['q75']]),\n",
    "                            'q90': float(values[pos['q90']]),\n",
    "                            'q95': float(values[pos['q95']]),\n",
    "                            'max': float(values[pos['max']])\n",
    "                        })\n",
    "                        \n",
    "                    elif len(values) == 1: # Seperating excluded rows between empty rows and incomplete rows\n",
    "                        skipped_rows.append({'course': course, 'year': values[pos['Year']], 'reason': 'no data in year'})\n",
    "                        \n",
    "                    else:\n",
    "                        skipped_rows.append({'course': course, 'year': values[pos['Year']], 'reason': 'incomplete data'})\n",
    "                \n",
    "                            \n",
    "                i += 1  # Moving to next line\n",
    "\n",
    "            mark_data.extend(course_data)\n",
    "            excluded_data.extend(skipped_rows)\n",
    "\n",
    "        else:\n",
    "            i += 1  # Moving to next line\n",
    "\n",
    "    return mark_data, excluded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8abc121-2e8a-4308-8712-5f8fb989a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all PDFs in the folder\n",
    "def process_pdfs(pdf_folder):\n",
    "    all_data = []\n",
    "    all_excl_data = []\n",
    "    for i, pdf_file in enumerate(pdf_files, 1):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        print(f'Processing ({i}/{total_files}): {pdf_file}...', end='\\r', flush=True)\n",
    "\n",
    "        # Extracting text from the PDF\n",
    "        text = extract_pdf_text(pdf_path)\n",
    "        \n",
    "        # Extracting department code and name\n",
    "        department_code, department_name = extract_department_code(text)\n",
    "\n",
    "        # Extracting mark summary data\n",
    "        mark_data, excluded_data = extract_marksummary(text, department_code, department_name)\n",
    "        \n",
    "        # Appending data\n",
    "        all_data.extend(mark_data)\n",
    "        all_excl_data.extend(excluded_data)\n",
    "\n",
    "    # Converting data\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df_excl = pd.DataFrame(all_excl_data)\n",
    "    \n",
    "    return df, df_excl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f68ad052-eef9-46b8-a7f6-87d6eb446991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (20/20): DS-results-2023-24-All-Sittings.pdf...\n",
      "Data scraping complete, 1934 rows of data extracted.\n",
      "\n",
      "516 rows deleted due to empty rows for years prior to introduction of new modules.\n",
      "49 rows deleted due to missing values resulting in misalignment.\n"
     ]
    }
   ],
   "source": [
    "# Scraping all PDFs & sorting data\n",
    "df, df_excl = process_pdfs(pdf_folder)\n",
    "df = df.sort_values(by=['code', 'year'], ascending=[True, True])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "empty_rows = len(df_excl[df_excl['reason'] == 'no data in year'])\n",
    "misaligned_rows = len(df_excl[df_excl['reason'] == 'incomplete data'])\n",
    "\n",
    "print('\\n'+f'Data scraping complete, {len(df)} rows of data extracted.'+'\\n')\n",
    "print(f'{empty_rows} rows deleted due to empty rows for years prior to introduction of new modules.')\n",
    "print(f'{misaligned_rows} rows deleted due to missing values resulting in misalignment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce9ad7c1-27f9-4795-840f-e77596af5c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes seperated and saved as CSV files\n"
     ]
    }
   ],
   "source": [
    "# Separating modules and department data\n",
    "departments_df = df[df['code'].str.len() == 2]\n",
    "departments_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "modules_df = df[df['code'].str.len() > 2]\n",
    "modules_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Saving DataFrames to CSV files\n",
    "modules_df.to_csv(\"data/modules/marks_summary_modules.csv\", index=False)\n",
    "departments_df.to_csv(\"data/departments/marks_summary_departments.csv\", index=False)\n",
    "\n",
    "print('Dataframes seperated and saved as CSV files')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.5"
=======
   "version": "3.10.16"
>>>>>>> 26a5f4117b6437ba9e2ecadcad5bba342f11dc61
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
