{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7078834c",
   "metadata": {},
   "source": [
    "# 1. Data Sourcing\n",
    "\n",
    "To effectively address our research question — how undergraduate students at LSE might strategically select courses and degrees that offer relatively easier pathways to achieving high grades — we first need to acquire a range of publicly available data from LSE's online resources. This initial phase involves identifying relevant data sources, scraping and collecting data, exploring and cleaning said datasets from faulty and irrelevant information, as well as preparing and structuring the datasets for downstream processing and analysis.\n",
    "\n",
    "The data sourcing process was divided into collecting two core datasets: (1) degree programme data, including recommended modules and application statistics, and (2) grade distributions by module, obtained from 5-year departmental PDF reports. These datasets need to be collected through targeted web scraping and manual extraction methods, considering the limitations of unstructured or semi-structured web formats.\n",
    "\n",
    "\n",
    "### Data of interest\n",
    "We specifically focused on the following data elements:\n",
    "1. **Undergraduate Degrees 2024/25:**\n",
    "    * Degree Names (of all available programmes)\n",
    "    * Module Information\n",
    "        * Mandatory Modules (per year)\n",
    "        * Outside Module Options\n",
    "    * Extra Information\n",
    "        * A-level requirements\n",
    "        * Application stats (volume, intake & acceptance rate)\n",
    "        * Tuition fees\n",
    "        * Median salary post-graduation\n",
    "<br>\n",
    "<br>\n",
    "2. **Undergraduate Modules 2024/25:**\n",
    "    * Module Codes (of all available courses)\n",
    "    * Grade distributions\n",
    "        * Grade summary statistics (mean, median, standard deviation, min, max, quartiles)\n",
    "        * Classified grade distributions (Number of 1st, 2:1s, 2:2s, 3rds, fails)\n",
    "    * Module Selection Criteria\n",
    "        * Prerequisites for courses\n",
    "        * Mutually exclusive courses\n",
    "    * Extra Information\n",
    "        * Number of enrolled students\n",
    "        * Average class sizes\n",
    "        * Capsizes (if applicable)\n",
    "        * Units of courses\n",
    "        * Responsible Departments\n",
    "\n",
    "### Data sources\n",
    "This data can be found on the following LSE websites:\n",
    "1. **Degree Information and Application Statistics**\n",
    "<br> *URL: https://www.lse.ac.uk/Programmes/Search-courses*\n",
    "<br> (22 pages of degree programmes with individual programme pages containing module, entry, and application data)\n",
    "\n",
    "2. **Module Grade Distributions**\n",
    "<br> *URL: https://info.lse.ac.uk/staff/divisions/academic-registrars-division/systems/what-we-do/internal/degree-and-course-results*\n",
    "<br> (includes departmental PDF files with annual module-grade data from 2019 - 2024)\n",
    "\n",
    "3. **Course Guide and Module Metadata:** \n",
    "<br> *URL: https://www.lse.ac.uk/resources/calendar2024-2025/courseGuides/undergraduate.htm*\n",
    "<br> (Contains course unit values, prerequisites, exclusions, departments, and descriptions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b02d38f",
   "metadata": {},
   "source": [
    "## 1.1. Degree Data Scraping\n",
    "\n",
    "The first phase of data acquisition involved extracting relevant degree programme information from the LSE Degree Search platform. This site lists all available undergraduate programmes for the 2024/25 academic year across 22 paginated results. Each programme contains a detail page with structured data including:\n",
    "\n",
    "* Programme title and UCAS code\n",
    "* Recommended modules (year-wise, compulsory and optional)\n",
    "* A-level entry requirements\n",
    "* Application statistics (number of applicants, number of offers, and intake)\n",
    "* Tuition fees (Home and International)\n",
    "* Career outcomes (median salary post-graduation, if listed)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930bdbf",
   "metadata": {},
   "source": [
    "### 1.1.1. Degree webpage URL scraping\n",
    "\n",
    "We first need to scrape the hyperlinks to all undergraduate degrees listed on the LSE Degree Search platform, which we will store in the programme_links list for further data extraction in later steps. Sorting through all 22 paginated result pages of the degree catalogue is achieved through extending the *Search-course* website's URL with it's page index and looping the link extraction through every results page. Within this loop the code uses requests to fetch each page’s HTML content and employs BeautifulSoup to parse the HTML and extract all anchor tags. Only links that correspond to undergraduate programme pages — identified by its URL pattern — are selected and converted into full URLs. These links are stored in the programme_links list for further data extraction in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1263ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 22/22...\n",
      "Total undergraduate programmes found: 42\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "base_url = 'https://www.lse.ac.uk'\n",
    "courses_url = 'https://www.lse.ac.uk/Programmes/Search-courses?pageIndex='\n",
    "\n",
    "# Getting links to websites of all undergraduate programme\n",
    "programme_links = []\n",
    "\n",
    "for page in range(1, 23):\n",
    "    print(f'Scraping page {page}/22...', end='\\r', flush=True)\n",
    "    url = f\"{courses_url}{page}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if href.startswith('/study-at-lse/undergraduate/'):\n",
    "            full_url = base_url + href\n",
    "            programme_links.append(full_url)\n",
    "\n",
    "print('\\n'+ f'Total undergraduate programmes found: {len(programme_links)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab9f4a",
   "metadata": {},
   "source": [
    "### 1.1.2. Degree data scraping - Function\n",
    "\n",
    "With the list of programme URLs collected in the previous step, the next step is to extract structured information from each individual degree page. To achieve this, we define a function called scrape_programme_data, which takes a single URL as input and returns a dictionary of the degree’s attributes.\n",
    "\n",
    "Before writing the function, it was necessary to manually inspect several programme webpages and their underlying HTML structures using developer tools and by examining the parsed output from BeautifulSoup. This allowed us to locate where elements of interest are stored within the HTML tree and identify consistent CSS selectors or tags we could use to extract the relevant data.\n",
    "\n",
    "**Targeted elements:**\n",
    "* **Degree name and department:** Retrieved from the page’s `<h1><span>` heading.\n",
    "* **Course structure per academic year:** Modules are listed in div blocks identified by unique `#year-x` IDs. These were iterated over to capture the course codes year-by-year.\n",
    "* **Entry requirements:** A-level requirements are stored under a specific element with the ID `#alevels`, and typically follow a paragraph structure.\n",
    "* **Application statistics:** These appear as stylized bullet-point figures under the `\"Your Application\"` section and include number of applications, intake, and acceptance ratio.\n",
    "* **Cost information:** Undergraduate home fees are usually listed in a paragraph block under the fees section and were extracted using a regular expression to capture pound-amounts `(£)`. Due to inconsistencies in the storage of overseas fees for various degrees, we opted to exclude this information from our dataset.\n",
    "* **Median graduate salary:** If available, this is shown in the `\"Graduate Destinations\"` section.\n",
    "\n",
    "The result of each function call is a dictionary containing all scraped data points, which can be appended to a list or converted into a DataFrame for further cleaning and analysis.\n",
    "\n",
    "*Note, since LSE webpages occasionally returned temporary server errors (status code 500) during scraping, we added a status code check at the start of the function to skip any pages that could not be successfully accessed. This ensures our scraping process remains robust and continues running without interruption.*\n",
    "\n",
    "*Further important observation is, that while the module lists are meant to only include all mandatory set courses for each year, in reality at times they include optional courses that are still listed on a degree's website as official course recommendations. Due to major inconsistencies among the storage of such recommended course options on the various websites, we are forced to later manually update these lists to exclude such instances and truly only reflect set mandatory courses for each degree.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c98f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extraxting data of interest from websites\n",
    "def scrape_programme_data(url):\n",
    "    res = requests.get(url)\n",
    "    if res.status_code != 200: # Needed to include due to temporary type 500 errors occuring when loading websites\n",
    "        print(f\"⚠️ Skipping {url} — status code {res.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Degree\n",
    "    course = soup.select_one('h1 > span').get_text(strip=True)\n",
    "    data['degree'] = course\n",
    "\n",
    "    # A-level requirement\n",
    "    alevel_elem = soup.select_one('#alevels > div > p')\n",
    "    alevel_text = alevel_elem.get_text(strip=True).split(maxsplit=1)\n",
    "    data['a_lvl_req'] = alevel_text[0].strip(',')\n",
    "    if len(alevel_text) > 1: data['a_lvl_extra'] = alevel_text[1]\n",
    "    else: data['a_lvl_extra'] = None\n",
    "\n",
    "    # Modules (looping through years)\n",
    "    data['modules_y1'] = []\n",
    "    data['modules_y2'] = []\n",
    "    data['modules_y3'] = []\n",
    "    data['modules_y4'] = []\n",
    "    for year in range(1, 5):  # assuming up to Year 3\n",
    "        modules = soup.select(f'#year-{year} div.code')\n",
    "        for module in modules:\n",
    "            code = module.get_text(strip=True)\n",
    "            if year == 1:\n",
    "                data['modules_y1'].append(code)\n",
    "            elif year == 2:\n",
    "                data['modules_y2'].append(code)\n",
    "            elif year == 3:\n",
    "                data['modules_y3'].append(code)\n",
    "            elif year == 4:\n",
    "                data['modules_y4'].append(code)\n",
    "        \n",
    "    # Applications statistics\n",
    "    nr_apps = soup.select_one(\"#your-application__overview .block--applications .stats\")\n",
    "    if nr_apps: data['nr_applications'] = nr_apps.get_text(strip=True)\n",
    "    else: data['nr_applications'] = None\n",
    "        \n",
    "    intake = soup.select_one(\"#your-application__overview .block--places .stats\")\n",
    "    if intake: data['intake'] = intake.get_text(strip=True)\n",
    "    else: data['intake'] = None\n",
    "        \n",
    "    ratio = soup.select_one(\"#your-application__overview .block--ratio .stats\")\n",
    "    if ratio: data['ratio'] = ratio.get_text(strip=True)\n",
    "    else: data['ratio'] = None\n",
    "\n",
    "    # Fees\n",
    "    home_fee_text = soup.select_one('#fees-and-funding__home p').get_text(strip=True)\n",
    "    data['home_fee'] = re.search(r'£[\\d,]+', home_fee_text).group()\n",
    "\n",
    "    # Median Salary\n",
    "    salary = soup.select_one('#graduate-destinations__overview .salary')\n",
    "    if salary: data['median_salary'] = salary.get_text(strip=True)\n",
    "    else: data['median_salary'] = None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03054648",
   "metadata": {},
   "source": [
    "### 1.1.3. Degree data scraping - Application\n",
    "\n",
    "We can now loop through each undergraduate degree URL and apply our previously defined scraping function to extract relevant data. Successfully scraped data is stored in a list, while any pages that failed to load (due to temporary server errors) are skipped and counted. This gives us a complete, structured dataset from the available programme pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf7fa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping https://www.lse.ac.uk/study-at-lse/undergraduate/bsc-economic-history — status code 500conomics\n",
      "Scraping 42/42: https://www.lse.ac.uk/study-at-lse/undergraduate/llb-bachelor-of-lawsogyehavioural-scienced-with-politicsss\n",
      "Data scraping complete\n",
      "\n",
      "1 programmes skipped in data extraction, due to website loading error (500).\n"
     ]
    }
   ],
   "source": [
    "# Applying function on all websites\n",
    "degrees_data = []\n",
    "skipped_urls = 0\n",
    "\n",
    "for i, url in enumerate(programme_links):\n",
    "    print(f\"Scraping {i+1}/{len(programme_links)}: {url}\", end='\\r', flush=True)\n",
    "    info = scrape_programme_data(url)\n",
    "    if info is None:\n",
    "        skipped_urls += 1\n",
    "    else:\n",
    "        degrees_data.append(info)\n",
    "\n",
    "print('\\nData scraping complete\\n')\n",
    "if skipped_urls >= 1:\n",
    "    print(f'{skipped_urls} programmes skipped in data extraction, due to website loading error (500).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad8423",
   "metadata": {},
   "source": [
    "### 1.1.4. Cleaning & Structuring Data\n",
    "\n",
    "After scraping, we remove any `None` entries from the list—these correspond to non-responsive or failed websites. We then convert the cleaned list of dictionaries into a pandas DataFrame. This tabular format allows for easier inspection, manipulation, and analysis of the degree data moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512dcdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>degree</th>\n",
       "      <th>a_lvl_req</th>\n",
       "      <th>a_lvl_extra</th>\n",
       "      <th>modules_y1</th>\n",
       "      <th>modules_y2</th>\n",
       "      <th>modules_y3</th>\n",
       "      <th>modules_y4</th>\n",
       "      <th>nr_applications</th>\n",
       "      <th>intake</th>\n",
       "      <th>ratio</th>\n",
       "      <th>home_fee</th>\n",
       "      <th>median_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BA Anthropology and Law</td>\n",
       "      <td>AAB</td>\n",
       "      <td>None</td>\n",
       "      <td>[LL141, AN100, AN101, LL142, LL108, LL100, LL1...</td>\n",
       "      <td>[AN253, AN379, LL106, LL143, LL200]</td>\n",
       "      <td>[LL276]</td>\n",
       "      <td>[]</td>\n",
       "      <td>250</td>\n",
       "      <td>20</td>\n",
       "      <td>13:1</td>\n",
       "      <td>£9,535</td>\n",
       "      <td>£34,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BA Geography</td>\n",
       "      <td>AAA</td>\n",
       "      <td>None</td>\n",
       "      <td>[GY100, GY140, GY144, LSE100]</td>\n",
       "      <td>[GY245, GY246, GY212, GY204, GY206, GY207]</td>\n",
       "      <td>[GY350]</td>\n",
       "      <td>[]</td>\n",
       "      <td>377</td>\n",
       "      <td>38</td>\n",
       "      <td>10:1</td>\n",
       "      <td>£9,535</td>\n",
       "      <td>£35,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BA History</td>\n",
       "      <td>AAA</td>\n",
       "      <td>None</td>\n",
       "      <td>[HY120, LSE100, EH101, HY113, HY116, HY118]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[HY300]</td>\n",
       "      <td>[]</td>\n",
       "      <td>503</td>\n",
       "      <td>58</td>\n",
       "      <td>9:1</td>\n",
       "      <td>£9,535</td>\n",
       "      <td>£35,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    degree a_lvl_req a_lvl_extra  \\\n",
       "0  BA Anthropology and Law       AAB        None   \n",
       "1             BA Geography       AAA        None   \n",
       "2               BA History       AAA        None   \n",
       "\n",
       "                                          modules_y1  \\\n",
       "0  [LL141, AN100, AN101, LL142, LL108, LL100, LL1...   \n",
       "1                      [GY100, GY140, GY144, LSE100]   \n",
       "2        [HY120, LSE100, EH101, HY113, HY116, HY118]   \n",
       "\n",
       "                                   modules_y2 modules_y3 modules_y4  \\\n",
       "0         [AN253, AN379, LL106, LL143, LL200]    [LL276]         []   \n",
       "1  [GY245, GY246, GY212, GY204, GY206, GY207]    [GY350]         []   \n",
       "2                                          []    [HY300]         []   \n",
       "\n",
       "  nr_applications intake ratio home_fee median_salary  \n",
       "0             250     20  13:1   £9,535       £34,500  \n",
       "1             377     38  10:1   £9,535       £35,000  \n",
       "2             503     58   9:1   £9,535       £35,000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning data from non-responsive websites & converting to Dataframe\n",
    "degrees_data_clean = [d for d in degrees_data if d is not None]\n",
    "degrees_df = pd.DataFrame(degrees_data_clean)\n",
    "degrees_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660ebc0",
   "metadata": {},
   "source": [
    "### 1.1.5. Exporting the Data\n",
    "\n",
    "To preserve our cleaned dataset and enable easy reuse in future analysis steps, we save the DataFrame as a CSV file in our project directory. This allows us to avoid re-scraping the web every time we need the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac113dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved as a CSV file\n"
     ]
    }
   ],
   "source": [
    "# Saving as CSV file\n",
    "degrees_df.to_csv('data/degrees/programme_data.csv', index=False)\n",
    "print('Data has been saved as a CSV file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968a08d",
   "metadata": {},
   "source": [
    "This CSV file now forms a foundational part of our analysis, offering essential context for identifying potential patterns in course difficulty, grade distributions, and academic outcomes across different degree programmes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20bd608",
   "metadata": {},
   "source": [
    "## 1.2. Module Grade Distribution Scraping\n",
    "\n",
    "To complement our degree-level data and gain more insight into what makes a course “easy” or high-scoring, we now turn to the course-level grade distributions. These statistics are published annually by each department at LSE in the form of PDF documents, which include a breakdown of student performance in each undergraduate module — typically showing distribution statistics and frequencies such as mean and median grades, as well as classified results (e.g., 1sts, 2:1s, etc.) for the last 5 years running.\n",
    "\n",
    "Unfortunately, the LSE webpage where these PDFs are hosted is behind a login portal that requires student credentials. Since it is not publicly accessible and protected against automated scraping, we manually downloaded the full set of course results PDFs across all departments and stored them locally in the *data/modules* folder. This ensures we can still extract and analyze the grade data programmatically.\n",
    "\n",
    "The goal of this section is to loop through each of these PDFs, parse out the relevant statistics for each undergraduate module, and clean them into a structured format suitable for analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c292c",
   "metadata": {},
   "source": [
    "### 1.2.1. Identifying Departments PDF's\n",
    "We start by looping through all files and identifying all relevant PDF files in our folder, allowing us to extract the module-level data needed for our broader analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb23b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# Identifying all PDFs\n",
    "pdf_folder = 'data/modules'\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "total_files = len(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91fd09",
   "metadata": {},
   "source": [
    "### 1.2.2. Extracting text from PDF\n",
    "\n",
    "Unlike scraping data from HTML websites — where structured elements like tags, classes, and IDs help us pinpoint exactly where data is stored — the structure of PDFs is often less consistent and not inherently designed for data extraction. As such, we needed to use the specialized Python library `pdfplumber`. This allows us to read and parse text content from PDF files while preserving the layout and line structure of the original documents.\n",
    "\n",
    "The following function reads every page of a given PDF and concatenates all extracted text into a single string, preparing it for further pattern-based filtering and analysis.\n",
    "\n",
    "*Note, at times the parsing encountered insignificant layout or font issues, resulting in persistent warning messages displayed - eventhough the output was perfectly fine. To deal with this and keep our output clean and readable, we imported the contextlib and io libraries. These let us suppress standard error messages during the PDF processing step. This workaround ensures that we can extract the text content reliably while ignoring any non-critical warnings cluttering the output.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de61eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF using pdfplumber\n",
    "def extract_pdf_text(pdf_path):\n",
    "    all_text = ''\n",
    "    with contextlib.redirect_stderr(io.StringIO()): # Use of AI to avoid warning messages\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    all_text += page_text + '\\n'\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74183807",
   "metadata": {},
   "source": [
    "### 1.2.3. Identifying Departments\n",
    "\n",
    "To begin parsing the content of each module PDF, we first needed to identify which department the module data belongs to. To do this, we created a function that scans the extracted PDF text line by line, searching for a pattern that consistently appears across files:\n",
    "\"*Department (XY) course results*\".\n",
    "\n",
    "Due to rare inconsistencies in the pattern of a course code (e.g. courses like ST101A or EC2C3 deviating from the typical AB123 pattern), we used the `re` library and AI to help write a generic regular expression that looks for a department name followed by a short code in parentheses and hence can generically match this format flexibly across departments and various course codes.\n",
    "\n",
    "We also used the `.group()`-function to extract the matching parts of the string (i.e. the department name and code). This function was something we discovered and learned to apply through AI assistance.\n",
    "\n",
    "The final result is a tuple containing the department code and department name, which we’ll use to label and organize the extracted data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5845086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract department name & code\n",
    "def extract_department_code(text):\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        match = re.search(r'([A-Za-z]+) \\(([A-Za-z]{2,4})\\) course results', line) # Use of AI to generate generic re code that identifies string\n",
    "        if match:\n",
    "            department_name = match.group(1) # Use of AI to learn about .group() function\n",
    "            department_code = match.group(2)\n",
    "            break\n",
    "    return department_code, department_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2926a2",
   "metadata": {},
   "source": [
    "### 1.2.4. Collecting Grade Distribution Statistics\n",
    "\n",
    "Next, we focus on parsing the actual Marksummary tables from each PDF. These tables contain statistical data such as mean, median, standard deviation of marks, as well as minimum and maximum marks and percentile values about student performance across individual courses and academic years. This data allows us to assess performance patterns across departments and over time and are the key part of our module-level analysis.\n",
    "\n",
    "**Approach to Extraction**\n",
    "\n",
    "We extract the data by defining a function that uses a while loop to parse through each PDF line-by-line using an index variable i, looking for the start of tables marked by the consistent header string *'Year marks mean sd'*. Once the header is found, we:\n",
    "* store the column names and map them to their respective indices\n",
    "* check for a course code, often found a few lines below the table, formatted like *AB123:Marksummary*\n",
    "* loop through each row, parsing values and adding them to a structured list mark_data (if the row is complete and aligns with the header)\n",
    "* keep track of excluded rows in excluded_data, especially those that are either empty or misaligned.\n",
    "\n",
    "This setup allows us to extract data even when PDFs contain multiple tables or have slightly inconsistent formatting.\n",
    "\n",
    "\n",
    "**Challenges Encountered and Fixes**\n",
    "\n",
    "One major issue we ran into was dealing with incomplete or misaligned table rows. These rows typically arise when the PDF text parser encounters blank cells in the original table—commonly seen when there are 0 values (e.g., 0 students received a fail grade). Unfortunately, when using pdfplumber (or any other PDF parser), these blank cells are not interpreted as 0, but instead are skipped altogether, causing the rest of the row to shift left, misaligning values with the headers and hence having the total number of entries in a row to fall short of the header length.\n",
    "\n",
    "This issue was especially common in the second table (called *'Gradesummary'*) that follows the Marksummary for each course, which contains the degree classification frequencies (e.g., # & % of students who got 1st, 2:1, 2:2, fail). Despite extensive attempts using both extensive online research and AI suggestions, we were not able to reliably parse this table due to the unpredictability of missing values and their impact on row structure.\n",
    "\n",
    "Thus, we made the decision to exclude the second table from our dataset. While unfortunate — since classification frequencies offer valuable and arguably more interesting insight — this decision was necessary to maintain the integrity of our dataset. The Marksummary statistics, by contrast, are consistently populated (as statistical summaries like mean, median, and standard deviation always require numerical input) and hence were extracted successfully in most cases. Including only rows where the lenght of values matches the lenght of the header, fully ensures that our dataset is reliable and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03829f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Marksummary tables from text\n",
    "def extract_marksummary(text, department_code, department_name):\n",
    "    lines = text.split('\\n')\n",
    "    mark_data = []\n",
    "    excluded_data = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        # Identifying table headers\n",
    "        if line.startswith('Year marks mean sd'):\n",
    "            header = line.split()\n",
    "            pos = {col: idx for idx, col in enumerate(header)}\n",
    "\n",
    "            # Identifying course code (at bottom of table)\n",
    "            course = department_code # Setting department code as default\n",
    "            for k in range(1, 7):\n",
    "                if i + k < len(lines):\n",
    "                    match = re.search(r'([A-Z0-9]+):Marksummary', lines[i + k])\n",
    "                    if match:\n",
    "                        course = match.group(1)\n",
    "                        break\n",
    "\n",
    "            # Moving to first data row (skipping header)\n",
    "            i += 1\n",
    "            course_data = []\n",
    "            skipped_rows = []\n",
    "            \n",
    "            while i < len(lines):\n",
    "                line = lines[i].strip()\n",
    "                # Break when encountering table title (at bottom of each table)\n",
    "                if re.match(r'([A-Z0-9]+):Marksummary', line) or line.startswith('MarksbyYear'):\n",
    "                    break\n",
    "\n",
    "                # Parsing data\n",
    "                if line:\n",
    "                    values = line.split()\n",
    "                    if len(values) == len(pos): # cleaning data from incomplete and misaligned rows due to missing values\n",
    "                        course_data.append({\n",
    "                            'department': department_name,\n",
    "                            'code': course,\n",
    "                            'year': values[pos['Year']],\n",
    "                            'marks': int(values[pos['marks']]),\n",
    "                            'mean': float(values[pos['mean']]),\n",
    "                            'sd': float(values[pos['sd']]),\n",
    "                            'min': float(values[pos['min']]),\n",
    "                            'q10': float(values[pos['q10']]),\n",
    "                            'q25': float(values[pos['q25']]),\n",
    "                            'median': float(values[pos['median']]),\n",
    "                            'IQR': float(values[pos['IQR']]),\n",
    "                            'q75': float(values[pos['q75']]),\n",
    "                            'q90': float(values[pos['q90']]),\n",
    "                            'q95': float(values[pos['q95']]),\n",
    "                            'max': float(values[pos['max']])\n",
    "                        })\n",
    "                        \n",
    "                    elif len(values) == 1: # Seperating excluded rows between empty rows and incomplete rows\n",
    "                        skipped_rows.append({'course': course, 'year': values[pos['Year']], 'reason': 'no data in year'})\n",
    "                        \n",
    "                    else:\n",
    "                        skipped_rows.append({'course': course, 'year': values[pos['Year']], 'reason': 'incomplete data'})\n",
    "                \n",
    "                            \n",
    "                i += 1  # Moving to next line\n",
    "\n",
    "            mark_data.extend(course_data)\n",
    "            excluded_data.extend(skipped_rows)\n",
    "\n",
    "        else:\n",
    "            i += 1  # Moving to next line\n",
    "\n",
    "    return mark_data, excluded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7364304",
   "metadata": {},
   "source": [
    "### 1.2.5. PDF Processing & Application\n",
    "\n",
    "Finally, we need to bring together all previously defined steps and apply them to every PDF in the specified folder with the following function. For each file, the mark summary statistics are parsed from the text, and valid rows are appended to a main data list, while excluded or malformed rows are collected separately. Lastly, both datasets are returned as DataFrames, ready for subsequent processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cc5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all PDFs in the folder\n",
    "def process_pdfs(pdf_folder):\n",
    "    all_data = []\n",
    "    all_excl_data = []\n",
    "    for i, pdf_file in enumerate(pdf_files, 1):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        print(f'Processing ({i}/{total_files}): {pdf_file}...', end='\\r', flush=True)\n",
    "\n",
    "        # Extracting text from the PDF\n",
    "        text = extract_pdf_text(pdf_path)\n",
    "        \n",
    "        # Extracting department code and name\n",
    "        department_code, department_name = extract_department_code(text)\n",
    "\n",
    "        # Extracting mark summary data\n",
    "        mark_data, excluded_data = extract_marksummary(text, department_code, department_name)\n",
    "        \n",
    "        # Appending data\n",
    "        all_data.extend(mark_data)\n",
    "        all_excl_data.extend(excluded_data)\n",
    "\n",
    "    # Converting data\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df_excl = pd.DataFrame(all_excl_data)\n",
    "    \n",
    "    return df, df_excl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a27804",
   "metadata": {},
   "source": [
    "### 1.2.6. Sorting Dataframe & Identifying Excluded Rows\n",
    "\n",
    "Once all PDFs have been processed, the resulting data is sorted by course code and academic year for easier readability and analysis. The DataFrame index is reset to ensure consistency after sorting. In addition, we report how many rows were excluded due to either being completely empty (often corresponding to years before a course was introduced) or misaligned (typically caused by missing values within a table row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab8fdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (20/20): DS-results-2023-24-All-Sittings.pdf...\n",
      "Data scraping complete, 1934 rows of data extracted.\n",
      "\n",
      "516 rows deleted due to empty rows for years prior to introduction of new modules.\n",
      "49 rows deleted due to missing values resulting in misalignment.\n"
     ]
    }
   ],
   "source": [
    "# Scraping all PDFs & sorting data\n",
    "df, df_excl = process_pdfs(pdf_folder)\n",
    "df = df.sort_values(by=['code', 'year'], ascending=[True, True])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "empty_rows = len(df_excl[df_excl['reason'] == 'no data in year'])\n",
    "misaligned_rows = len(df_excl[df_excl['reason'] == 'incomplete data'])\n",
    "\n",
    "print('\\n'+f'Data scraping complete, {len(df)} rows of data extracted.'+'\\n')\n",
    "print(f'{empty_rows} rows deleted due to empty rows for years prior to introduction of new modules.')\n",
    "print(f'{misaligned_rows} rows deleted due to missing values resulting in misalignment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7057adc1",
   "metadata": {},
   "source": [
    "### 1.2.7. Saving Seperated Dataframes\n",
    "\n",
    "In the final step of the module data scraping process, we distinguish between department-level and individual module-level data based on the length of the course code,creating two separate DataFrames for clearer organization and analysis. Each is then saved as a CSV file in the appropriate directory, ensuring our data is both accessible and structured for the next phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778e8e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes seperated and saved as CSV files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>code</th>\n",
       "      <th>year</th>\n",
       "      <th>marks</th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>min</th>\n",
       "      <th>q10</th>\n",
       "      <th>q25</th>\n",
       "      <th>median</th>\n",
       "      <th>IQR</th>\n",
       "      <th>q75</th>\n",
       "      <th>q90</th>\n",
       "      <th>q95</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>AC100</td>\n",
       "      <td>2019/20</td>\n",
       "      <td>116</td>\n",
       "      <td>76.9</td>\n",
       "      <td>9.2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>72.8</td>\n",
       "      <td>79.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>83.2</td>\n",
       "      <td>86.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>AC100</td>\n",
       "      <td>2020/21</td>\n",
       "      <td>145</td>\n",
       "      <td>67.4</td>\n",
       "      <td>10.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>AC100</td>\n",
       "      <td>2021/22</td>\n",
       "      <td>114</td>\n",
       "      <td>65.8</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>81.7</td>\n",
       "      <td>85.7</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   department   code     year  marks  mean    sd   min   q10   q25  median  \\\n",
       "0  Accounting  AC100  2019/20    116  76.9   9.2  45.0  65.0  72.8    79.0   \n",
       "1  Accounting  AC100  2020/21    145  67.4  10.4  32.0  54.0  61.0    69.0   \n",
       "2  Accounting  AC100  2021/22    114  65.8  15.6   0.0  47.3  58.0    68.0   \n",
       "\n",
       "    IQR   q75   q90   q95   max  \n",
       "0  10.5  83.2  86.0  87.0  91.0  \n",
       "1  14.0  75.0  79.0  81.0  85.0  \n",
       "2  19.0  77.0  81.7  85.7  88.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating modules and department data\n",
    "departments_df = df[df['code'].str.len() == 2]\n",
    "departments_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "modules_df = df[df['code'].str.len() > 2]\n",
    "modules_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Saving DataFrames to CSV files\n",
    "modules_df.to_csv(\"data/modules/marks_summary_modules.csv\", index=False)\n",
    "departments_df.to_csv(\"data/departments/marks_summary_departments.csv\", index=False)\n",
    "\n",
    "print('Dataframes seperated and saved as CSV files')\n",
    "modules_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608cc878",
   "metadata": {},
   "source": [
    "## 1.3. Module Extra Info Scraping\n",
    "\n",
    "To complement the statistical data we extracted from the departmental mark summaries, we also collect additional contextual information about each module from the official LSE course guides.\n",
    "\n",
    "Unlike the PDFs, the course guide data is published on a public-facing HTML website, which means we are able to scrape this data automatically using Python. Hence we can scrape the master course guide index page to retrieve links to all available undergraduate modules for the academic year 2024/25 and afterwards visit each course’s individual page to extract key pieces of structured information.\n",
    "\n",
    "**Elements of extra information of interest:**\n",
    "* Full Course Name\n",
    "* Responsible Department\n",
    "* Prerequisites for taking the course\n",
    "* Total number of students enrolled (in the previous year)\n",
    "* Average class size\n",
    "* Course Cap Size (if applicable)\n",
    "* Academic unit (i.e. how many credits it's worth)\n",
    "\n",
    "These data points allow us to better contextualize our quantitative mark statistics by providing metadata on how each module is structured.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49141714",
   "metadata": {},
   "source": [
    "### 1.3.1. Collecting Course Guides\n",
    "\n",
    "To start, we scrape the main undergraduate course guide page to collect URLs for all individual course modules. After sending a request to the page and parsing it with BeautifulSoup, we extract all anchor tags within tables — where we found the course links grouped by department through HTML inspection. Then we can filter for valid guide links (those starting with *\"../courseGuides/\"*) and reconstruct each full URL using the known base path. The resulting `course_links` list contains the direct URLs to all undergraduate course pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fb6c9b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 568 course guide links.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "base_url = 'https://www.lse.ac.uk/resources/calendar2024-2025/courseGuides'\n",
    "guide_url = f'{base_url}/undergraduate.htm'\n",
    "\n",
    "# Getting all links to course guides\n",
    "response = requests.get(guide_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Finding all tables (each course is stored in departments table)\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "course_links = []\n",
    "\n",
    "for table in tables:\n",
    "    for a_tag in table.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        if href.startswith('../courseGuides/'):\n",
    "            full_url = base_url + href.split('../courseGuides')[1]\n",
    "            course_links.append(full_url)\n",
    "\n",
    "print(f'Found {len(course_links)} course guide links.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebe0eb",
   "metadata": {},
   "source": [
    "### 1.3.2. Identifying Extra Info\n",
    "\n",
    "Next, we define a function that takes a single course guide URL and scrapes all data of interest from it. After extracting the course code and title from the `<title>` tag, it then accesses the `“Key Facts”` section and pulls out relevant fields like department name, student numbers, class size, whether the course is capped, and unit value. Each of these is identified by the prefix of the `<p>` tag’s content (e.g. “Department:”, “Value:”, etc.).\n",
    "\n",
    "The second part of the function focuses on identifying prerequisite modules. It searches the `\"Prerequisites\"` section and uses a regular expression to extract any valid LSE course codes. Note, that once again due to rare inconsistencies in the pattern of a course code (e.g. courses like ST101A or EC2C3 deviating from the typical AB123 pattern), we used AI to create a generic regular expression that includes such irregularities. To further avoid circular references, we filter out duplicates and the course itself. All extracted data is stored in a dictionary, ready to be appended to a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6b963f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_course_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title = soup.find('title').get_text().split(maxsplit=1)\n",
    "    code = title[0]\n",
    "    course = title[1]\n",
    "    \n",
    "    data = {'code': code, 'course': course}\n",
    "    \n",
    "    # Extracting Information from Key Facts Section\n",
    "    key_facts_section = soup.find('div', id='keyFacts-Content')\n",
    "    items = key_facts_section.find_all('p')\n",
    "        \n",
    "    for item in items:\n",
    "        text = item.get_text(strip=True)\n",
    "    \n",
    "        if text.startswith('Department'):\n",
    "            data['department'] = text.split(':')[1].strip()\n",
    "        elif text.startswith('Total students'):\n",
    "            data['total_students'] = text.split(':')[1].strip()\n",
    "        elif text.startswith('Average class size'):\n",
    "            data['avg_class_size'] = text.split(':')[1].strip()\n",
    "        elif text.startswith('Capped'):\n",
    "            data['capped'] = text.split(':')[1].strip()\n",
    "        elif text.startswith(\"Value:\"):\n",
    "            data['units'] = text.split('Value:')[1].strip()\n",
    "    \n",
    "    # Extracting Information from Prerequisits Section\n",
    "    prereq_div = soup.find('div', id='preRequisites-Content')\n",
    "    prereqs = set()\n",
    "    \n",
    "    if prereq_div:\n",
    "        text = prereq_div.get_text(separator=' ', strip=True)\n",
    "        prereqs.update(re.findall(r'\\b([A-Z]{2,}\\d+[A-Z]?)\\b', text)) # Use of AI to create generic re string\n",
    "        prereqs.discard(code)\n",
    "\n",
    "    data['prerequisites'] = list(prereqs)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c816cc6",
   "metadata": {},
   "source": [
    "### 1.3.3. Scraping Extra Info\n",
    "\n",
    "Now that we have set up the scraping function, we can run the scraper across all previously collected course guide links, printing progress along the way. The extracted data is then converted into a dataframe, as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "68b47bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 568/568: https://www.lse.ac.uk/resources/calendar2024-2025/courseGuides/ST/2024_ST360.htmmm\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>course</th>\n",
       "      <th>department</th>\n",
       "      <th>total_students</th>\n",
       "      <th>avg_class_size</th>\n",
       "      <th>capped</th>\n",
       "      <th>units</th>\n",
       "      <th>prerequisites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AC205</td>\n",
       "      <td>Intermediate Financial Accounting</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>No</td>\n",
       "      <td>Half Unit</td>\n",
       "      <td>[AC102, AC105]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AC206</td>\n",
       "      <td>Intermediate Management Accounting</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>No</td>\n",
       "      <td>Half Unit</td>\n",
       "      <td>[AC106, AC103]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AC311</td>\n",
       "      <td>Results Accountability and Management Control ...</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>118</td>\n",
       "      <td>39</td>\n",
       "      <td>No</td>\n",
       "      <td>Half Unit</td>\n",
       "      <td>[AC103, AC200, AC312, AC100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    code                                             course  department  \\\n",
       "4  AC205                  Intermediate Financial Accounting  Accounting   \n",
       "5  AC206                 Intermediate Management Accounting  Accounting   \n",
       "6  AC311  Results Accountability and Management Control ...  Accounting   \n",
       "\n",
       "  total_students avg_class_size capped      units  \\\n",
       "4    Unavailable    Unavailable     No  Half Unit   \n",
       "5    Unavailable    Unavailable     No  Half Unit   \n",
       "6            118             39     No  Half Unit   \n",
       "\n",
       "                  prerequisites  \n",
       "4                [AC102, AC105]  \n",
       "5                [AC106, AC103]  \n",
       "6  [AC103, AC200, AC312, AC100]  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the scraper\n",
    "all_course_data = []\n",
    "\n",
    "for i, url in enumerate(course_links):\n",
    "    print(f\"Scraping {i+1}/{len(course_links)}: {url}\", end='\\r', flush=True)\n",
    "    course_data = extract_course_data(url)\n",
    "    all_course_data.append(course_data)\n",
    "\n",
    "# Converting to DataFrame\n",
    "modules_facts = pd.DataFrame(all_course_data)\n",
    "modules_facts[modules_facts['prerequisites'].astype(bool)].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e8471",
   "metadata": {},
   "source": [
    "### 1.3.4. Cleaning and Formatting\n",
    "\n",
    "Next we clean and standardize the scraped course data to prepare it for analysis. Unit values are converted from strings to numerical floats. Missing values in student counts and average class sizes are handled by replacing \"Unavailable\" with NaN and casting to floats. The capped field is converted to a boolean or numeric value depending on whether a cap exists, extracting the numerical cap where present. Finally, the 'code' column is set as the index for the DataFrame to uniquely identify each module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dbde05fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>department</th>\n",
       "      <th>total_students</th>\n",
       "      <th>avg_class_size</th>\n",
       "      <th>capped</th>\n",
       "      <th>units</th>\n",
       "      <th>prerequisites</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AC102</th>\n",
       "      <td>Elements of Financial Accounting</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>564.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC103</th>\n",
       "      <td>Elements of Management Accounting, Financial M...</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>256.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC105</th>\n",
       "      <td>Introduction to Financial Accounting</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>115.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  course  department  \\\n",
       "code                                                                   \n",
       "AC102                   Elements of Financial Accounting  Accounting   \n",
       "AC103  Elements of Management Accounting, Financial M...  Accounting   \n",
       "AC105               Introduction to Financial Accounting  Accounting   \n",
       "\n",
       "       total_students  avg_class_size capped  units prerequisites  \n",
       "code                                                               \n",
       "AC102           564.0            15.0  False    0.5            []  \n",
       "AC103           256.0            18.0  False    0.5            []  \n",
       "AC105           115.0            39.0  False    0.5            []  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Data\n",
    "modules_facts['units'] = modules_facts['units'].map({'One Unit': 1.0, 'Half Unit': 0.5, 'Non-credit bearing': 0.0})\n",
    "modules_facts['total_students'] = modules_facts['total_students'].replace('Unavailable', np.nan).astype(float)\n",
    "modules_facts['avg_class_size'] = modules_facts['avg_class_size'].replace('Unavailable', np.nan).astype(float)\n",
    "modules_facts.loc[modules_facts['capped'].str.startswith('No'), 'capped'] = False\n",
    "modules_facts.loc[modules_facts['capped'] != False, 'capped'] = modules_facts.loc[modules_facts['capped'] != False, 'capped'].str.split(' ').str[1].str.strip('()').astype(int)\n",
    "modules_facts.set_index('code', inplace=True)\n",
    "modules_facts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af253c9",
   "metadata": {},
   "source": [
    "### 1.3.5. Mutually Exclusive Modules\n",
    "\n",
    "We are further interested in courses that are mutually exclusive to other courses, as we need to confirm valid course selection later on in our analysis. Thus, we manually created a CSV file containing all mutually exclusive course pairs, which we load into a dataframe, setting the 'code' column as the index for easier lookups. Printing out the dataframe info, confirms that there are 62 mutually exclusive pairs.\n",
    "\n",
    "*Note, that manual creation of this file was necessary due to major complications in the way information on mutual exclusive courses was stored in the course guides, with pairs often only mentioned in textblocks and their exclusivity explained with 'and' or 'or' statements, of which reliable data extraction would have superseeded our coding abilities.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "34210b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 62 entries, DS101A to ST330\n",
      "Data columns (total 1 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   mutually_exclusive  62 non-null     object\n",
      "dtypes: object(1)\n",
      "memory usage: 992.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "mutually_exclusive = pd.read_csv('data/modules/mutual_exclusive.csv')\n",
    "mutually_exclusive.set_index('code', inplace=True)\n",
    "mutually_exclusive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3edf97",
   "metadata": {},
   "source": [
    "### 1.3.6. Merging Dataframes\n",
    "\n",
    "With our mutually exclusive data, we can now merge the two DataFrames by performing a left join based on the `code` column. This ensures that the mutually exclusive course data is added as a new column in the `modules_facts` DataFrame, with matching values for the corresponding courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "02929539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>department</th>\n",
       "      <th>total_students</th>\n",
       "      <th>avg_class_size</th>\n",
       "      <th>capped</th>\n",
       "      <th>units</th>\n",
       "      <th>prerequisites</th>\n",
       "      <th>mutually_exclusive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DS101A</th>\n",
       "      <td>Fundamentals of Data Science</td>\n",
       "      <td>Data Science Institute</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>DS101W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS101W</th>\n",
       "      <td>Fundamentals of Data Science</td>\n",
       "      <td>Data Science Institute</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>DS101A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS105A</th>\n",
       "      <td>Data for Data Science</td>\n",
       "      <td>Data Science Institute</td>\n",
       "      <td>60.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>DS105W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS105W</th>\n",
       "      <td>Data for Data Science</td>\n",
       "      <td>Data Science Institute</td>\n",
       "      <td>52.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>DS104A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EC1A3</th>\n",
       "      <td>Microeconomics I</td>\n",
       "      <td>Economics</td>\n",
       "      <td>751.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>EC1A5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              course              department  total_students  \\\n",
       "code                                                                           \n",
       "DS101A  Fundamentals of Data Science  Data Science Institute            28.0   \n",
       "DS101W  Fundamentals of Data Science  Data Science Institute             8.0   \n",
       "DS105A         Data for Data Science  Data Science Institute            60.0   \n",
       "DS105W         Data for Data Science  Data Science Institute            52.0   \n",
       "EC1A3               Microeconomics I               Economics           751.0   \n",
       "\n",
       "        avg_class_size capped  units prerequisites mutually_exclusive  \n",
       "code                                                                   \n",
       "DS101A            11.0  False    0.5            []             DS101W  \n",
       "DS101W             5.0  False    0.5            []             DS101A  \n",
       "DS105A            13.0  False    0.5            []             DS105W  \n",
       "DS105W            10.0  False    0.5            []             DS104A  \n",
       "EC1A3             16.0  False    0.5            []              EC1A5  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add mutually exclusive course data as a new column\n",
    "modules_facts = modules_facts.join(mutually_exclusive, how='left')\n",
    "\n",
    "# Display updated DataFrame\n",
    "modules_facts[modules_facts['mutually_exclusive'].notna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d576b",
   "metadata": {},
   "source": [
    "### 1.3.7. Validating Cleaned Data\n",
    "\n",
    "To ensure data integrity before storage, we inspect the structure and completeness of the `modules_facts` DataFrame using `.info()`. The output shows that all modules have complete values for most columns, with missing values in `total_students` and `avg_class_size` due to unreported statistics in the course guides—these omissions are not critical for our analysis.\n",
    "\n",
    "All entries contain valid values in the prerequisites column, confirming that our scraper consistently extracted prerequisite information, even where no prerequisites were listed (resulting in empty lists).\n",
    "\n",
    "We also identify that only 56 out of the 62 mutually exclusive course pairs were successfully merged. By comparing the indices and manually inspecting the course guides, we confirm that the 6 missing course codes are absent from the course guides altogether, suggesting they correspond to outdated or discontinued modules. Since these are no longer offered, they are irrelevant for our current analysis and can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2c337c86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 569 entries, AC102 to ST360\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   course              569 non-null    object \n",
      " 1   department          569 non-null    object \n",
      " 2   total_students      468 non-null    float64\n",
      " 3   avg_class_size      461 non-null    float64\n",
      " 4   capped              569 non-null    object \n",
      " 5   units               569 non-null    float64\n",
      " 6   prerequisites       569 non-null    object \n",
      " 7   mutually_exclusive  56 non-null     object \n",
      "dtypes: float64(3), object(5)\n",
      "memory usage: 56.2+ KB\n",
      "None\n",
      "\n",
      "Missing codes: {'SO208', 'SO308', 'SO210', 'FM213', 'DS202', 'SO224'}\n",
      "6 out of 62 total entries are missing.\n"
     ]
    }
   ],
   "source": [
    "# Counting rows containing NaN values\n",
    "print(modules_facts.info())\n",
    "\n",
    "# Identifying missing mutual exclusive pairs\n",
    "missing_codes = set(mutually_exclusive.index) - set(modules_facts.index)\n",
    "print('\\n'+\"Missing codes:\", missing_codes)\n",
    "print(f\"{len(missing_codes)} out of {len(mutually_exclusive)} total entries are missing.\")\n",
    "\n",
    "# Saving to CSV\n",
    "modules_facts.to_csv('data/modules/modules_key_facts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474760cb",
   "metadata": {},
   "source": [
    "## 1.4. Outside Options\n",
    "\n",
    "Finally, we need to compile a list of standard outside options — elective modules available to most undergraduate students across various LSE programmes. While there is no centralized list explicitly outlining all shared outside options, manual inspection across different programme regulations (especially BSc Finance) reveals a consistent set of course codes frequently permitted as electives. We use the BSc Finance programme page to extract this list as a reliable proxy, which will be crucial for later analysis of elective pathways and degree flexibility.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35faca7",
   "metadata": {},
   "source": [
    "### 1.4.1. Identifying Outside Options List \n",
    "\n",
    "We begin by inspecting BSc Finance undergraduate programme page and parsing its HTML content with BeautifulSoup, identifying that this page contains a table listing the standard outside options allowed for most degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9b7ee06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.lse.ac.uk/resources/calendar/programmeRegulations/undergraduate/2024/BScFinance.htm'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e6612",
   "metadata": {},
   "source": [
    "### 1.4.2. Locating & Extracting Course Codes\n",
    "\n",
    "We iterate through all `<h3>` tags to locate the one linking to the Undergraduate Outside Options List, then identify the surrounding `<div>` and extract all course codes from the associated table. Upon inspection of the resulting dataframe we can count 374 elective modules that can be chosen outside a student’s main department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "769e2038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 374 entries, AC102 to ST330\n",
      "Empty DataFrame\n"
     ]
    }
   ],
   "source": [
    "for h3 in soup.find_all(\"h3\"):\n",
    "    a_tag = h3.find(\"a\")\n",
    "    if a_tag and \"Undergraduate Outside Options List\" in a_tag.text:\n",
    "        header = h3\n",
    "        break\n",
    "\n",
    "outside_course_codes = []\n",
    "\n",
    "parent_div = header.find_parent(\"div\")\n",
    "table = parent_div.find(\"table\")\n",
    "links = table.find_all(\"a\", href=True)\n",
    "for row in table.find_all(\"tr\"):\n",
    "    link = row.find(\"a\", href=True)\n",
    "    if link:\n",
    "        course_code = link.text.strip()\n",
    "        outside_course_codes.append(course_code)\n",
    "        \n",
    "outside_options = pd.DataFrame(outside_course_codes)\n",
    "outside_options.columns = ['code']\n",
    "outside_options.set_index('code', inplace=True)\n",
    "outside_options.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f500d3",
   "metadata": {},
   "source": [
    "### 1.4.3. Merging Dataframes\n",
    "\n",
    "We enrich the `outside_options` table by joining it with the `modules_facts` DataFrame to save a final dataset that solely contains all extra information on outside options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "48b3cb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>department</th>\n",
       "      <th>total_students</th>\n",
       "      <th>avg_class_size</th>\n",
       "      <th>capped</th>\n",
       "      <th>units</th>\n",
       "      <th>prerequisites</th>\n",
       "      <th>mutually_exclusive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AC102</th>\n",
       "      <td>Elements of Financial Accounting</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>564.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC103</th>\n",
       "      <td>Elements of Management Accounting, Financial M...</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>256.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC205</th>\n",
       "      <td>Intermediate Financial Accounting</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[AC102, AC105]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC206</th>\n",
       "      <td>Intermediate Management Accounting</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[AC106, AC103]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AC311</th>\n",
       "      <td>Results Accountability and Management Control ...</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>118.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>[AC103, AC200, AC312, AC100]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  course  department  \\\n",
       "code                                                                   \n",
       "AC102                   Elements of Financial Accounting  Accounting   \n",
       "AC103  Elements of Management Accounting, Financial M...  Accounting   \n",
       "AC205                  Intermediate Financial Accounting  Accounting   \n",
       "AC206                 Intermediate Management Accounting  Accounting   \n",
       "AC311  Results Accountability and Management Control ...  Accounting   \n",
       "\n",
       "       total_students  avg_class_size capped  units  \\\n",
       "code                                                  \n",
       "AC102           564.0            15.0  False    0.5   \n",
       "AC103           256.0            18.0  False    0.5   \n",
       "AC205             NaN             NaN  False    0.5   \n",
       "AC206             NaN             NaN  False    0.5   \n",
       "AC311           118.0            39.0  False    0.5   \n",
       "\n",
       "                      prerequisites mutually_exclusive  \n",
       "code                                                    \n",
       "AC102                            []                NaN  \n",
       "AC103                            []                NaN  \n",
       "AC205                [AC102, AC105]                NaN  \n",
       "AC206                [AC106, AC103]                NaN  \n",
       "AC311  [AC103, AC200, AC312, AC100]                NaN  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outside_options = outside_options.join(modules_facts, how='left')\n",
    "outside_options.to_csv('data/degrees/ug_outside_options.csv', index=False)\n",
    "outside_options.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b175df",
   "metadata": {},
   "source": [
    "## 1.5. Creating Complete Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
