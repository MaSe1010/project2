{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c226534c",
   "metadata": {},
   "source": [
    "# Is there difficulty variation between degrees at LSE?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe151e60",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "1. **Introduction**\n",
    "\n",
    "* 1.1\tMotivation\n",
    "* 1.2\tObjectives and Research Questions\n",
    "* 1.3\tOriginality\n",
    "* 1.4\tBrief Overview of Project Process\n",
    "\n",
    "2. **Data Acquisition and Preparation**\n",
    "* 2.1\tDegree Data Scraping\n",
    "* 2.2\tModule Grade Distributions Scraping\n",
    "* 2.3\tModules Extra Information Scraping\n",
    "\n",
    "3. **Degree Data Preparation and Analysis**\n",
    "* 3.1.\tDegree Data Exploration and Wrangling\n",
    "* 3.2.\tDegree Data Analysis and Visualisation\n",
    "\n",
    "4. **Module Data Preparation**\n",
    "* 4.1.\tWhy did we choose these degrees\n",
    "* 4.2.\tThe general method employed\n",
    "* 4.3.\tDiscovering the Designs\n",
    "   * 4.31) Finance\n",
    "   * 4.32) Economics\n",
    "   * 4.33) International Relations\n",
    "   * 4.34) Politics and Economics\n",
    "   * 4.35) Physchological and Behavioural Sciences\n",
    "\n",
    "5. **Module-wide Analysis**\n",
    "<br>\n",
    "\n",
    "6. **Summary and Conclusion**\n",
    "* 6.1.\tSummary of findings\n",
    "* 6.2.\tConclusions based on findings\n",
    "* 6.3.\tPotential analysis limitations\n",
    "* 6.4.\tReferences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced9ff0",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b02fd",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "As undergraduate students who have recently undergone the UCAS application process, we aim to verify whether studying a specific degree offers a higher chance of achieving higher grades and, subsequently, a higher graduate salary compared to studying another degree. Finally, we are examining the discrepancy of grade distribution within each degree, varying for different combinations of optional modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6ffcb",
   "metadata": {},
   "source": [
    "## Overview of Project Process\n",
    "\n",
    "\n",
    "This introductory-level study will include creating a data frame, which allows for possible module combinations that a student can take within a degree. This will be restricted to the recommended modules, under which we will then work out grade distributions by module. Module selection will be based on specific criteria of interest that are thoroughly explained through the analysis. \n",
    "\n",
    "Following the module selection, we will conduct a hypothesis test, by accounting for the standard error within each module, to see if it is statistically possible to artificially \n",
    "increase your chances of receiving higher degree marks, and if this leads to accessing the increased rewards of higher graduation grades.\n",
    "\n",
    "Finally, we will examine on a quantitative and/or qualitative note different statistics that could play a part in each module and degree grade attainment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef4bb2",
   "metadata": {},
   "source": [
    "## Originality\n",
    "\n",
    "Looking at LSE’s freedom of information page as well as any other publicly available data, there is no similar database or study that has been done, looking at the possible commutations accounting for standard errors to produce a statistical test on this data. This is the primary aspect which we consider to be original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb8afc",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "* Is it possible to ‘game the system’ in terms of degree choice and module choice to unlock greater graduation outcomes?\n",
    "* Does the grade attainment difference between the “hard” and “easy” degree designs depend on optionality, application acceptance rate, and/or graduation salary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da8f9a",
   "metadata": {},
   "source": [
    "# 2. Data Sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044750f3",
   "metadata": {},
   "source": [
    "To effectively address our research question — how undergraduate students at LSE might strategically select courses and degrees that offer relatively easier pathways to achieving high grades — we first need to acquire a range of publicly available data from LSE's online resources. This initial phase involves identifying relevant data sources, scraping and collecting data, exploring and cleaning said datasets from faulty and irrelevant information, as well as preparing and structuring the datasets for downstream processing and analysis.\n",
    "\n",
    "The data sourcing process was divided into collecting two core datasets: (1) degree programme data, including recommended modules and application statistics, and (2) grade distributions by module, obtained from 5-year departmental PDF reports. These datasets need to be collected through targeted web scraping and manual extraction methods, considering the limitations of unstructured or semi-structured web formats.\n",
    "\n",
    "\n",
    "### Data of interest\n",
    "We specifically focused on the following data elements:\n",
    "1. **Undergraduate Degrees 2024/25:**\n",
    "    * Degree Names (of all available programmes)\n",
    "    * Module Information\n",
    "        * Mandatory Modules (per year)\n",
    "        * Outside Module Options\n",
    "    * Extra Information\n",
    "        * A-level requirements\n",
    "        * Application stats (volume, intake & acceptance rate)\n",
    "        * Tuition fees\n",
    "        * Median salary post-graduation\n",
    "<br>\n",
    "<br>\n",
    "2. **Undergraduate Modules 2024/25:**\n",
    "    * Module Codes (of all available courses)\n",
    "    * Grade distributions\n",
    "        * Grade summary statistics (mean, median, standard deviation, min, max, quartiles)\n",
    "        * Classified grade distributions (Number of 1st, 2:1s, 2:2s, 3rds, fails)\n",
    "    * Module Selection Criteria\n",
    "        * Prerequisites for courses\n",
    "        * Mutually exclusive courses\n",
    "    * Extra Information\n",
    "        * Number of enrolled students\n",
    "        * Average class sizes\n",
    "        * Capsizes (if applicable)\n",
    "        * Units of courses\n",
    "        * Responsible Departments\n",
    "\n",
    "### Data sources\n",
    "This data can be found on the following LSE websites:\n",
    "1. **Degree Information and Application Statistics**\n",
    "<br> *URL: https://www.lse.ac.uk/Programmes/Search-courses*\n",
    "<br> (22 pages of degree programmes with individual programme pages containing module, entry, and application data)\n",
    "\n",
    "2. **Module Grade Distributions**\n",
    "<br> *URL: https://info.lse.ac.uk/staff/divisions/academic-registrars-division/systems/what-we-do/internal/degree-and-course-results*\n",
    "<br> (includes departmental PDF files with annual module-grade data from 2019 - 2024)\n",
    "\n",
    "3. **Course Guide and Module Metadata:** \n",
    "<br> *URL: https://www.lse.ac.uk/resources/calendar2024-2025/courseGuides/undergraduate.htm*\n",
    "<br> (Contains course unit values, prerequisites, exclusions, departments, and descriptions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b86f2",
   "metadata": {},
   "source": [
    "## 2.1. Degree Data Scraping\n",
    "\n",
    "The first phase of data acquisition involved extracting relevant degree programme information from the LSE Degree Search platform. This site lists all available undergraduate programmes for the 2024/25 academic year across 22 paginated results. Each programme contains a detail page with structured data including:\n",
    "\n",
    "* Programme title and UCAS code\n",
    "* Recommended modules (year-wise, compulsory and optional)\n",
    "* A-level entry requirements\n",
    "* Application statistics (number of applicants, number of offers, and intake)\n",
    "* Tuition fees (Home and International)\n",
    "* Career outcomes (median salary post-graduation, if listed)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347cf7b",
   "metadata": {},
   "source": [
    "### 2.1.1. Degree webpage URL scraping\n",
    "\n",
    "We first need to scrape the hyperlinks to all undergraduate degrees listed on the LSE Degree Search platform, which we will store in the programme_links list for further data extraction in later steps. Sorting through all 22 paginated result pages of the degree catalogue is achieved through extending the *Search-course* website's URL with it's page index and looping the link extraction through every results page. Within this loop the code uses requests to fetch each page’s HTML content and employs BeautifulSoup to parse the HTML and extract all anchor tags. Only links that correspond to undergraduate programme pages — identified by its URL pattern — are selected and converted into full URLs. These links are stored in the programme_links list for further data extraction in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420f57a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 22/22...\n",
      "Total undergraduate programmes found: 42\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "base_url = 'https://www.lse.ac.uk'\n",
    "courses_url = 'https://www.lse.ac.uk/Programmes/Search-courses?pageIndex='\n",
    "\n",
    "# Getting links to websites of all undergraduate programme\n",
    "programme_links = []\n",
    "\n",
    "for page in range(1, 23):\n",
    "    print(f'Scraping page {page}/22...', end='\\r', flush=True)\n",
    "    url = f\"{courses_url}{page}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if href.startswith('/study-at-lse/undergraduate/'):\n",
    "            full_url = base_url + href\n",
    "            programme_links.append(full_url)\n",
    "\n",
    "print('\\n'+ f'Total undergraduate programmes found: {len(programme_links)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0b998",
   "metadata": {},
   "source": [
    "### 2.1.2. Degree data scraping - Function\n",
    "\n",
    "With the list of programme URLs collected in the previous step, the next step is to extract structured information from each individual degree page. To achieve this, we define a function called scrape_programme_data, which takes a single URL as input and returns a dictionary of the degree’s attributes.\n",
    "\n",
    "Before writing the function, it was necessary to manually inspect several programme webpages and their underlying HTML structures using developer tools and by examining the parsed output from BeautifulSoup. This allowed us to locate where elements of interest are stored within the HTML tree and identify consistent CSS selectors or tags we could use to extract the relevant data.\n",
    "\n",
    "**Targeted elements:**\n",
    "* **Degree name and department:** Retrieved from the page’s `<h1><span>` heading.\n",
    "* **Course structure per academic year:** Modules are listed in div blocks identified by unique `#year-x` IDs. These were iterated over to capture the course codes year-by-year.\n",
    "* **Entry requirements:** A-level requirements are stored under a specific element with the ID `#alevels`, and typically follow a paragraph structure.\n",
    "* **Application statistics:** These appear as stylized bullet-point figures under the `\"Your Application\"` section and include number of applications, intake, and acceptance ratio.\n",
    "* **Cost information:** Undergraduate home fees are usually listed in a paragraph block under the fees section and were extracted using a regular expression to capture pound-amounts `(£)`. Due to inconsistencies in the storage of overseas fees for various degrees, we opted to exclude this information from our dataset.\n",
    "* **Median graduate salary:** If available, this is shown in the `\"Graduate Destinations\"` section.\n",
    "\n",
    "The result of each function call is a dictionary containing all scraped data points, which can be appended to a list or converted into a DataFrame for further cleaning and analysis.\n",
    "\n",
    "*Note, since LSE webpages occasionally returned temporary server errors (status code 500) during scraping, we added a status code check at the start of the function to skip any pages that could not be successfully accessed. This ensures our scraping process remains robust and continues running without interruption.*\n",
    "\n",
    "*Further important observation is, that while the module lists are meant to only include all mandatory set courses for each year, in reality at times they include optional courses that are still listed on a degree's website as official course recommendations. Due to major inconsistencies among the storage of such recommended course options on the various websites, we are forced to later manually update these lists to exclude such instances and truly only reflect set mandatory courses for each degree.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe44235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extraxting data of interest from websites\n",
    "def scrape_programme_data(url):\n",
    "    res = requests.get(url)\n",
    "    if res.status_code != 200: # Needed to include due to temporary type 500 errors occuring when loading websites\n",
    "        print(f\"⚠️ Skipping {url} — status code {res.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    data = {}\n",
    "\n",
    "    # Degree\n",
    "    course = soup.select_one('h1 > span').get_text(strip=True)\n",
    "    data['degree'] = course\n",
    "\n",
    "    # A-level requirement\n",
    "    alevel_elem = soup.select_one('#alevels > div > p')\n",
    "    alevel_text = alevel_elem.get_text(strip=True).split(maxsplit=1)\n",
    "    data['a_lvl_req'] = alevel_text[0].strip(',')\n",
    "    if len(alevel_text) > 1: data['a_lvl_extra'] = alevel_text[1]\n",
    "    else: data['a_lvl_extra'] = None\n",
    "\n",
    "    # Modules (looping through years)\n",
    "    data['modules_y1'] = []\n",
    "    data['modules_y2'] = []\n",
    "    data['modules_y3'] = []\n",
    "    data['modules_y4'] = []\n",
    "    for year in range(1, 5):  # assuming up to Year 3\n",
    "        modules = soup.select(f'#year-{year} div.code')\n",
    "        for module in modules:\n",
    "            code = module.get_text(strip=True)\n",
    "            if year == 1:\n",
    "                data['modules_y1'].append(code)\n",
    "            elif year == 2:\n",
    "                data['modules_y2'].append(code)\n",
    "            elif year == 3:\n",
    "                data['modules_y3'].append(code)\n",
    "            elif year == 4:\n",
    "                data['modules_y4'].append(code)\n",
    "        \n",
    "    # Applications statistics\n",
    "    nr_apps = soup.select_one(\"#your-application__overview .block--applications .stats\")\n",
    "    if nr_apps: data['nr_applications'] = nr_apps.get_text(strip=True)\n",
    "    else: data['nr_applications'] = None\n",
    "        \n",
    "    intake = soup.select_one(\"#your-application__overview .block--places .stats\")\n",
    "    if intake: data['intake'] = intake.get_text(strip=True)\n",
    "    else: data['intake'] = None\n",
    "        \n",
    "    ratio = soup.select_one(\"#your-application__overview .block--ratio .stats\")\n",
    "    if ratio: data['ratio'] = ratio.get_text(strip=True)\n",
    "    else: data['ratio'] = None\n",
    "\n",
    "    # Fees\n",
    "    home_fee_text = soup.select_one('#fees-and-funding__home p').get_text(strip=True)\n",
    "    data['home_fee'] = re.search(r'£[\\d,]+', home_fee_text).group()\n",
    "\n",
    "    # Median Salary\n",
    "    salary = soup.select_one('#graduate-destinations__overview .salary')\n",
    "    if salary: data['median_salary'] = salary.get_text(strip=True)\n",
    "    else: data['median_salary'] = None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd4cc6",
   "metadata": {},
   "source": [
    "### 2.1.3. Degree data scraping - Application\n",
    "\n",
    "We can now loop through each undergraduate degree URL and apply our previously defined scraping function to extract relevant data. Successfully scraped data is stored in a list, while any pages that failed to load (due to temporary server errors) are skipped and counted. This gives us a complete, structured dataset from the available programme pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be7d061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 42/42: https://www.lse.ac.uk/study-at-lse/undergraduate/llb-bachelor-of-lawsogyehavioural-scienced-with-politicsss\n",
      "Data scraping complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying function on all websites\n",
    "degrees_data = []\n",
    "skipped_urls = 0\n",
    "\n",
    "for i, url in enumerate(programme_links):\n",
    "    print(f\"Scraping {i+1}/{len(programme_links)}: {url}\", end='\\r', flush=True)\n",
    "    info = scrape_programme_data(url)\n",
    "    if info is None:\n",
    "        skipped_urls += 1\n",
    "    else:\n",
    "        degrees_data.append(info)\n",
    "\n",
    "print('\\nData scraping complete\\n')\n",
    "if skipped_urls >= 1:\n",
    "    print(f'{skipped_urls} programmes skipped in data extraction, due to website loading error (500).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d33cc",
   "metadata": {},
   "source": [
    "### 2.1.4. Cleaning & Structuring Data\n",
    "\n",
    "After scraping, we remove any `None` entries from the list—these correspond to non-responsive or failed websites. We then convert the cleaned list of dictionaries into a pandas DataFrame. This tabular format allows for easier inspection, manipulation, and analysis of the degree data moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fa170f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>degree</th>\n",
       "      <th>a_lvl_req</th>\n",
       "      <th>a_lvl_extra</th>\n",
       "      <th>modules_y1</th>\n",
       "      <th>modules_y2</th>\n",
       "      <th>modules_y3</th>\n",
       "      <th>modules_y4</th>\n",
       "      <th>nr_applications</th>\n",
       "      <th>intake</th>\n",
       "      <th>ratio</th>\n",
       "      <th>home_fee</th>\n",
       "      <th>median_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BA Anthropology and Law</td>\n",
       "      <td>AAB</td>\n",
       "      <td>None</td>\n",
       "      <td>[LL141, AN100, AN101, LL142, LL108, LL100, LL1...</td>\n",
       "      <td>[AN253, AN379, LL106, LL143, LL200]</td>\n",
       "      <td>[LL276]</td>\n",
       "      <td>[]</td>\n",
       "      <td>250</td>\n",
       "      <td>20</td>\n",
       "      <td>13:1</td>\n",
       "      <td>£9,535</td>\n",
       "      <td>£34,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BA Geography</td>\n",
       "      <td>AAA</td>\n",
       "      <td>None</td>\n",
       "      <td>[GY100, GY140, GY144, LSE100]</td>\n",
       "      <td>[GY245, GY246, GY212, GY204, GY206, GY207]</td>\n",
       "      <td>[GY350]</td>\n",
       "      <td>[]</td>\n",
       "      <td>377</td>\n",
       "      <td>38</td>\n",
       "      <td>10:1</td>\n",
       "      <td>£9,535</td>\n",
       "      <td>£35,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BA History</td>\n",
       "      <td>AAA</td>\n",
       "      <td>None</td>\n",
       "      <td>[HY120, LSE100, EH101, HY113, HY116, HY118]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[HY300]</td>\n",
       "      <td>[]</td>\n",
       "      <td>503</td>\n",
       "      <td>58</td>\n",
       "      <td>9:1</td>\n",
       "      <td>£9,535</td>\n",
       "      <td>£35,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    degree a_lvl_req a_lvl_extra  \\\n",
       "0  BA Anthropology and Law       AAB        None   \n",
       "1             BA Geography       AAA        None   \n",
       "2               BA History       AAA        None   \n",
       "\n",
       "                                          modules_y1  \\\n",
       "0  [LL141, AN100, AN101, LL142, LL108, LL100, LL1...   \n",
       "1                      [GY100, GY140, GY144, LSE100]   \n",
       "2        [HY120, LSE100, EH101, HY113, HY116, HY118]   \n",
       "\n",
       "                                   modules_y2 modules_y3 modules_y4  \\\n",
       "0         [AN253, AN379, LL106, LL143, LL200]    [LL276]         []   \n",
       "1  [GY245, GY246, GY212, GY204, GY206, GY207]    [GY350]         []   \n",
       "2                                          []    [HY300]         []   \n",
       "\n",
       "  nr_applications intake ratio home_fee median_salary  \n",
       "0             250     20  13:1   £9,535       £34,500  \n",
       "1             377     38  10:1   £9,535       £35,000  \n",
       "2             503     58   9:1   £9,535       £35,000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning data from non-responsive websites & converting to Dataframe\n",
    "degrees_data_clean = [d for d in degrees_data if d is not None]\n",
    "degrees_df = pd.DataFrame(degrees_data_clean)\n",
    "degrees_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa540d",
   "metadata": {},
   "source": [
    "### 2.1.5. Exporting the Data\n",
    "\n",
    "To preserve our cleaned dataset and enable easy reuse in future analysis steps, we save the DataFrame as a CSV file in our project directory. This allows us to avoid re-scraping the web every time we need the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9aecb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved as a CSV file\n"
     ]
    }
   ],
   "source": [
    "# Saving as CSV file\n",
    "degrees_df.to_csv('data/degrees/programme_data.csv', index=False)\n",
    "print('Data has been saved as a CSV file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70d69a",
   "metadata": {},
   "source": [
    "This CSV file now forms a foundational part of our analysis, offering essential context for identifying potential patterns in course difficulty, grade distributions, and academic outcomes across different degree programmes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa061c99",
   "metadata": {},
   "source": [
    "### 2.1.6. Manual Correction\n",
    "\n",
    "As aforementioned, now that the CSV file is saved, as part of extensive exploration of the saved data, we noticed optional courses being included in the scraped lists of mandatory modules at times. Hence, we manually went through the `programme_data` CSV to update the `module_y1`, `module_y2`, and `module_y3` columns, and get rid of falsely included optional courses in those lists.\n",
    "\n",
    "The corrected file is saved as `programme_data_NDC.csv` under the same path, and is the file that will be used for further analysis later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb70d2e",
   "metadata": {},
   "source": [
    "## 2.2. Module Grade Distribution Scraping\n",
    "\n",
    "To complement our degree-level data and gain more insight into what makes a course “easy” or high-scoring, we now turn to the course-level grade distributions. These statistics are published annually by each department at LSE in the form of PDF documents, which include a breakdown of student performance in each undergraduate module — typically showing distribution statistics and frequencies such as mean and median grades, as well as classified results (e.g., 1sts, 2:1s, etc.) for the last 5 years running.\n",
    "\n",
    "Unfortunately, the LSE webpage where these PDFs are hosted is behind a login portal that requires student credentials. Since it is not publicly accessible and protected against automated scraping, we manually downloaded the full set of course results PDFs across all departments and stored them locally in the *data/modules* folder. This ensures we can still extract and analyze the grade data programmatically.\n",
    "\n",
    "The goal of this section is to loop through each of these PDFs, parse out the relevant statistics for each undergraduate module, and clean them into a structured format suitable for analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a3f83",
   "metadata": {},
   "source": [
    "### 2.2.1. Identifying Departments PDF's\n",
    "We start by looping through all files and identifying all relevant PDF files in our folder, allowing us to extract the module-level data needed for our broader analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984af3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# Identifying all PDFs\n",
    "pdf_folder = 'data/modules'\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "total_files = len(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be536fde",
   "metadata": {},
   "source": [
    "### 2.2.2. Extracting text from PDF\n",
    "\n",
    "Unlike scraping data from HTML websites — where structured elements like tags, classes, and IDs help us pinpoint exactly where data is stored — the structure of PDFs is often less consistent and not inherently designed for data extraction. As such, we needed to use the specialized Python library `pdfplumber`. This allows us to read and parse text content from PDF files while preserving the layout and line structure of the original documents.\n",
    "\n",
    "The following function reads every page of a given PDF and concatenates all extracted text into a single string, preparing it for further pattern-based filtering and analysis.\n",
    "\n",
    "*Note, at times the parsing encountered insignificant layout or font issues, resulting in persistent warning messages displayed - eventhough the output was perfectly fine. To deal with this and keep our output clean and readable, we imported the contextlib and io libraries. These let us suppress standard error messages during the PDF processing step. This workaround ensures that we can extract the text content reliably while ignoring any non-critical warnings cluttering the output.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f24d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF using pdfplumber\n",
    "def extract_pdf_text(pdf_path):\n",
    "    all_text = ''\n",
    "    with contextlib.redirect_stderr(io.StringIO()): # Use of AI to avoid warning messages\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    all_text += page_text + '\\n'\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3beea",
   "metadata": {},
   "source": [
    "### 2.2.3. Identifying Departments\n",
    "\n",
    "To begin parsing the content of each module PDF, we first needed to identify which department the module data belongs to. To do this, we created a function that scans the extracted PDF text line by line, searching for a pattern that consistently appears across files:\n",
    "\"*Department (XY) course results*\".\n",
    "\n",
    "Due to rare inconsistencies in the pattern of a course code (e.g. courses like ST101A or EC2C3 deviating from the typical AB123 pattern), we used the `re` library and AI to help write a generic regular expression that looks for a department name followed by a short code in parentheses and hence can generically match this format flexibly across departments and various course codes.\n",
    "\n",
    "We also used the `.group()`-function to extract the matching parts of the string (i.e. the department name and code). This function was something we discovered and learned to apply through AI assistance.\n",
    "\n",
    "The final result is a tuple containing the department code and department name, which we’ll use to label and organize the extracted data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b5f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract department name & code\n",
    "def extract_department_code(text):\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        match = re.search(r'([A-Za-z]+) \\(([A-Za-z]{2,4})\\) course results', line) # Use of AI to generate generic re code that identifies string\n",
    "        if match:\n",
    "            department_name = match.group(1) # Use of AI to learn about .group() function\n",
    "            department_code = match.group(2)\n",
    "            break\n",
    "    return department_code, department_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ccd67",
   "metadata": {},
   "source": [
    "### 2.2.4. Collecting Grade Distribution Statistics\n",
    "\n",
    "Next, we focus on parsing the actual Marksummary tables from each PDF. These tables contain statistical data such as mean, median, standard deviation of marks, as well as minimum and maximum marks and percentile values about student performance across individual courses and academic years. This data allows us to assess performance patterns across departments and over time and are the key part of our module-level analysis.\n",
    "\n",
    "**Approach to Extraction**\n",
    "\n",
    "We extract the data by defining a function that uses a while loop to parse through each PDF line-by-line using an index variable i, looking for the start of tables marked by the consistent header string *'Year marks mean sd'*. Once the header is found, we:\n",
    "* store the column names and map them to their respective indices\n",
    "* check for a course code, often found a few lines below the table, formatted like *AB123:Marksummary*\n",
    "* loop through each row, parsing values and adding them to a structured list mark_data (if the row is complete and aligns with the header)\n",
    "* keep track of excluded rows in excluded_data, especially those that are either empty or misaligned.\n",
    "\n",
    "This setup allows us to extract data even when PDFs contain multiple tables or have slightly inconsistent formatting.\n",
    "\n",
    "\n",
    "**Challenges Encountered and Fixes**\n",
    "\n",
    "One major issue we ran into was dealing with incomplete or misaligned table rows. These rows typically arise when the PDF text parser encounters blank cells in the original table—commonly seen when there are 0 values (e.g., 0 students received a fail grade). Unfortunately, when using pdfplumber (or any other PDF parser), these blank cells are not interpreted as 0, but instead are skipped altogether, causing the rest of the row to shift left, misaligning values with the headers and hence having the total number of entries in a row to fall short of the header length.\n",
    "\n",
    "This issue was especially common in the second table (called *'Gradesummary'*) that follows the Marksummary for each course, which contains the degree classification frequencies (e.g., # & % of students who got 1st, 2:1, 2:2, fail). Despite extensive attempts using both extensive online research and AI suggestions, we were not able to reliably parse this table due to the unpredictability of missing values and their impact on row structure.\n",
    "\n",
    "Thus, we made the decision to exclude the second table from our dataset. While unfortunate — since classification frequencies offer valuable and arguably more interesting insight — this decision was necessary to maintain the integrity of our dataset. The Marksummary statistics, by contrast, are consistently populated (as statistical summaries like mean, median, and standard deviation always require numerical input) and hence were extracted successfully in most cases. Including only rows where the lenght of values matches the lenght of the header, fully ensures that our dataset is reliable and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebccbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Marksummary tables from text\n",
    "def extract_marksummary(text, department_code, department_name):\n",
    "    lines = text.split('\\n')\n",
    "    mark_data = []\n",
    "    excluded_data = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        # Identifying table headers\n",
    "        if line.startswith('Year marks mean sd'):\n",
    "            header = line.split()\n",
    "            pos = {col: idx for idx, col in enumerate(header)}\n",
    "\n",
    "            # Identifying course code (at bottom of table)\n",
    "            course = department_code # Setting department code as default\n",
    "            for k in range(1, 7):\n",
    "                if i + k < len(lines):\n",
    "                    match = re.search(r'([A-Z0-9]+):Marksummary', lines[i + k])\n",
    "                    if match:\n",
    "                        course = match.group(1)\n",
    "                        break\n",
    "\n",
    "            # Moving to first data row (skipping header)\n",
    "            i += 1\n",
    "            course_data = []\n",
    "            skipped_rows = []\n",
    "            \n",
    "            while i < len(lines):\n",
    "                line = lines[i].strip()\n",
    "                # Break when encountering table title (at bottom of each table)\n",
    "                if re.match(r'([A-Z0-9]+):Marksummary', line) or line.startswith('MarksbyYear'):\n",
    "                    break\n",
    "\n",
    "                # Parsing data\n",
    "                if line:\n",
    "                    values = line.split()\n",
    "                    if len(values) == len(pos): # cleaning data from incomplete and misaligned rows due to missing values\n",
    "                        course_data.append({\n",
    "                            'department': department_name,\n",
    "                            'code': course,\n",
    "                            'year': values[pos['Year']],\n",
    "                            'marks': int(values[pos['marks']]),\n",
    "                            'mean': float(values[pos['mean']]),\n",
    "                            'sd': float(values[pos['sd']]),\n",
    "                            'min': float(values[pos['min']]),\n",
    "                            'q10': float(values[pos['q10']]),\n",
    "                            'q25': float(values[pos['q25']]),\n",
    "                            'median': float(values[pos['median']]),\n",
    "                            'IQR': float(values[pos['IQR']]),\n",
    "                            'q75': float(values[pos['q75']]),\n",
    "                            'q90': float(values[pos['q90']]),\n",
    "                            'q95': float(values[pos['q95']]),\n",
    "                            'max': float(values[pos['max']])\n",
    "                        })\n",
    "                        \n",
    "                    elif len(values) == 1: # Seperating excluded rows between empty rows and incomplete rows\n",
    "                        skipped_rows.append({'course': course, 'year': values[pos['Year']], 'reason': 'no data in year'})\n",
    "                        \n",
    "                    else:\n",
    "                        skipped_rows.append({'course': course, 'year': values[pos['Year']], 'reason': 'incomplete data'})\n",
    "                \n",
    "                            \n",
    "                i += 1  # Moving to next line\n",
    "\n",
    "            mark_data.extend(course_data)\n",
    "            excluded_data.extend(skipped_rows)\n",
    "\n",
    "        else:\n",
    "            i += 1  # Moving to next line\n",
    "\n",
    "    return mark_data, excluded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf992f2",
   "metadata": {},
   "source": [
    "### 2.2.5. PDF Processing & Application\n",
    "\n",
    "Finally, we need to bring together all previously defined steps and apply them to every PDF in the specified folder with the following function. For each file, the mark summary statistics are parsed from the text, and valid rows are appended to a main data list, while excluded or malformed rows are collected separately. Lastly, both datasets are returned as DataFrames, ready for subsequent processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e4369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all PDFs in the folder\n",
    "def process_pdfs(pdf_folder):\n",
    "    all_data = []\n",
    "    all_excl_data = []\n",
    "    for i, pdf_file in enumerate(pdf_files, 1):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        print(f'Processing ({i}/{total_files}): {pdf_file}...', end='\\r', flush=True)\n",
    "\n",
    "        # Extracting text from the PDF\n",
    "        text = extract_pdf_text(pdf_path)\n",
    "        \n",
    "        # Extracting department code and name\n",
    "        department_code, department_name = extract_department_code(text)\n",
    "\n",
    "        # Extracting mark summary data\n",
    "        mark_data, excluded_data = extract_marksummary(text, department_code, department_name)\n",
    "        \n",
    "        # Appending data\n",
    "        all_data.extend(mark_data)\n",
    "        all_excl_data.extend(excluded_data)\n",
    "\n",
    "    # Converting data\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df_excl = pd.DataFrame(all_excl_data)\n",
    "    \n",
    "    return df, df_excl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d280822",
   "metadata": {},
   "source": [
    "### 2.2.6. Sorting Dataframe & Identifying Excluded Rows\n",
    "\n",
    "Once all PDFs have been processed, the resulting data is sorted by course code and academic year for easier readability and analysis. The DataFrame index is reset to ensure consistency after sorting. In addition, we report how many rows were excluded due to either being completely empty (often corresponding to years before a course was introduced) or misaligned (typically caused by missing values within a table row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f4786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (10/20): LL-results-2023-24-All-Sittings.pdf...\r"
     ]
    }
   ],
   "source": [
    "# Scraping all PDFs & sorting data\n",
    "df, df_excl = process_pdfs(pdf_folder)\n",
    "df = df.sort_values(by=['code', 'year'], ascending=[True, True])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "empty_rows = len(df_excl[df_excl['reason'] == 'no data in year'])\n",
    "misaligned_rows = len(df_excl[df_excl['reason'] == 'incomplete data'])\n",
    "\n",
    "print('\\n'+f'Data scraping complete, {len(df)} rows of data extracted.'+'\\n')\n",
    "print(f'{empty_rows} rows deleted due to empty rows for years prior to introduction of new modules.')\n",
    "print(f'{misaligned_rows} rows deleted due to missing values resulting in misalignment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991068f6",
   "metadata": {},
   "source": [
    "### 2.2.7. Saving Seperated Dataframes\n",
    "\n",
    "In the final step of the module data scraping process, we distinguish between department-level and individual module-level data based on the length of the course code,creating two separate DataFrames for clearer organization and analysis. Each is then saved as a CSV file in the appropriate directory, ensuring our data is both accessible and structured for the next phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating modules and department data\n",
    "departments_df = df[df['code'].str.len() == 2]\n",
    "departments_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "modules_df = df[df['code'].str.len() > 2]\n",
    "modules_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Saving DataFrames to CSV files\n",
    "modules_df.to_csv(\"data/modules/marks_summary_modules.csv\", index=False)\n",
    "departments_df.to_csv(\"data/departments/marks_summary_departments.csv\", index=False)\n",
    "\n",
    "print('Dataframes seperated and saved as CSV files')\n",
    "print(f'{len(modules_df)} row entries')\n",
    "modules_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941605a",
   "metadata": {},
   "source": [
    "### 2.2.8. Filtering Current Courses\n",
    "\n",
    "For further refinement, we will be filtering our dataset solely for courses that have data on the academic year 2023/24, suggesting that they are active and available and thus relevant courses. We can see that with this we have removed 109 for further analysis irrelevant rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "marks_df = modules_df[modules_df['code'].isin(modules_df[modules_df['year'] == '2023/24']['code'].unique())]\n",
    "print(f'{len(marks_df)} row entries, {len(modules_df)-len(marks_df)} rows deleted.')\n",
    "marks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8ffab",
   "metadata": {},
   "source": [
    "### 2.2.9. Averages Across Years\n",
    "\n",
    "Lastly, we will group all data for each course by taking the averages of all the statistics over the years, using the following code. This will allow us to accurately assess the historic performance of students in each course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e629ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "summary_df = marks_df.groupby('code').apply(\n",
    "    lambda group: pd.Series({\n",
    "        'mean': np.average(group['mean']) if group['marks'].sum() > 0 else np.nan,\n",
    "        'q10': group['q10'].mean() if group['marks'].sum() > 0 else np.nan,\n",
    "        'Median': group['median'].mean() if group['marks'].sum() > 0 else np.nan,\n",
    "        'q90': group['q90'].mean() if group['marks'].sum() > 0 else np.nan,\n",
    "        'Pooled_SD': (\n",
    "            np.sqrt(((group['marks'] - 1) * group['sd'] ** 2).sum() / (group['marks'].sum() - len(group)))\n",
    "            if group['marks'].sum() - len(group) > 0 else np.nan),\n",
    "        'Department': group['department'].mode()[0] if not group['department'].mode().empty else np.nan\n",
    "    })).reset_index()\n",
    "summary_df.set_index('code', inplace=True)\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a2ecc",
   "metadata": {},
   "source": [
    "## 2.3. Module Extra Info Scraping\n",
    "\n",
    "To complement the statistical data we extracted from the departmental mark summaries, we also collect additional contextual information about each module from the official LSE course guides.\n",
    "\n",
    "Unlike the PDFs, the course guide data is published on a public-facing HTML website, which means we are able to scrape this data automatically using Python. Hence we can scrape the master course guide index page to retrieve links to all available undergraduate modules for the academic year 2024/25 and afterwards visit each course’s individual page to extract key pieces of structured information.\n",
    "\n",
    "**Elements of extra information of interest:**\n",
    "* Full Course Name\n",
    "* Responsible Department\n",
    "* Prerequisites for taking the course\n",
    "* Total number of students enrolled (in the previous year)\n",
    "* Average class size\n",
    "* Course Cap Size (if applicable)\n",
    "* Academic unit (i.e. how many credits it's worth)\n",
    "\n",
    "These data points allow us to better contextualize our quantitative mark statistics by providing metadata on how each module is structured.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde059c",
   "metadata": {},
   "source": [
    "### 2.3.1. Collecting Course Guides\n",
    "\n",
    "To start, we scrape the main undergraduate course guide page to collect URLs for all individual course modules. After sending a request to the page and parsing it with BeautifulSoup, we extract all anchor tags within tables — where we found the course links grouped by department through HTML inspection. Then we can filter for valid guide links (those starting with *\"../courseGuides/\"*) and reconstruct each full URL using the known base path. The resulting `course_links` list contains the direct URLs to all undergraduate course pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "base_url = 'https://www.lse.ac.uk/resources/calendar2024-2025/courseGuides'\n",
    "guide_url = f'{base_url}/undergraduate.htm'\n",
    "\n",
    "# Getting all links to course guides\n",
    "response = requests.get(guide_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Finding all tables (each course is stored in departments table)\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "course_links = []\n",
    "\n",
    "for table in tables:\n",
    "    for a_tag in table.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        if href.startswith('../courseGuides/'):\n",
    "            full_url = base_url + href.split('../courseGuides')[1]\n",
    "            course_links.append(full_url)\n",
    "\n",
    "print(f'Found {len(course_links)} course guide links.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af85e6",
   "metadata": {},
   "source": [
    "### 2.3.2. Identifying Extra Info\n",
    "\n",
    "Next, we define a function that takes a single course guide URL and scrapes all data of interest from it. After extracting the course code and title from the `<title>` tag, it then accesses the `“Key Facts”` section and pulls out relevant fields like department name, student numbers, class size, whether the course is capped, and unit value. Each of these is identified by the prefix of the `<p>` tag’s content (e.g. “Department:”, “Value:”, etc.).\n",
    "\n",
    "The second part of the function focuses on identifying prerequisite modules. It searches the `\"Prerequisites\"` section and uses a regular expression to extract any valid LSE course codes. Note, that once again due to rare inconsistencies in the pattern of a course code (e.g. courses like ST101A or EC2C3 deviating from the typical AB123 pattern), we used AI to create a generic regular expression that includes such irregularities. To further avoid circular references, we filter out duplicates and the course itself. All extracted data is stored in a dictionary, ready to be appended to a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_course_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title = soup.find('title').get_text().split(maxsplit=1)\n",
    "    code = title[0]\n",
    "    course = title[1]\n",
    "    \n",
    "    data = {'code': code, 'course': course}\n",
    "    \n",
    "    # Extracting Information from Key Facts Section\n",
    "    key_facts_section = soup.find('div', id='keyFacts-Content')\n",
    "    items = key_facts_section.find_all('p')\n",
    "        \n",
    "    for item in items:\n",
    "        text = item.get_text(strip=True)\n",
    "    \n",
    "        if text.startswith('Department'):\n",
    "            data['department'] = text.split(':')[1].strip()\n",
    "        elif text.startswith('Total students'):\n",
    "            data['total_students'] = text.split(':')[1].strip()\n",
    "        elif text.startswith('Average class size'):\n",
    "            data['avg_class_size'] = text.split(':')[1].strip()\n",
    "        elif text.startswith('Capped'):\n",
    "            data['capped'] = text.split(':')[1].strip()\n",
    "        elif text.startswith(\"Value:\"):\n",
    "            data['units_x'] = text.split('Value:')[1].strip()\n",
    "    \n",
    "    # Extracting Information from Prerequisits Section\n",
    "    prereq_div = soup.find('div', id='preRequisites-Content')\n",
    "    prereqs = set()\n",
    "    \n",
    "    if prereq_div:\n",
    "        text = prereq_div.get_text(separator=' ', strip=True)\n",
    "        prereqs.update(re.findall(r'\\b([A-Z]{2,}\\d+[A-Z]?)\\b', text)) # Use of AI to create generic re string\n",
    "        prereqs.discard(code)\n",
    "\n",
    "    data['prerequisites'] = list(prereqs)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc85291",
   "metadata": {},
   "source": [
    "### 2.3.3. Scraping Extra Info\n",
    "\n",
    "Now that we have set up the scraping function, we can run the scraper across all previously collected course guide links, printing progress along the way. The extracted data is then converted into a dataframe, as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1084e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the scraper\n",
    "all_course_data = []\n",
    "\n",
    "for i, url in enumerate(course_links):\n",
    "    print(f\"Scraping {i+1}/{len(course_links)}: {url}\", end='\\r', flush=True)\n",
    "    course_data = extract_course_data(url)\n",
    "    all_course_data.append(course_data)\n",
    "\n",
    "# Converting to DataFrame\n",
    "modules_facts = pd.DataFrame(all_course_data)\n",
    "modules_facts[modules_facts['prerequisites'].astype(bool)].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4131fc",
   "metadata": {},
   "source": [
    "### 2.3.4. Cleaning and Formatting\n",
    "\n",
    "Next we clean and standardize the scraped course data to prepare it for analysis. Unit values are converted from strings to numerical floats. Missing values in student counts and average class sizes are handled by replacing \"Unavailable\" with NaN and casting to floats. The capped field is converted to a boolean or numeric value depending on whether a cap exists, extracting the numerical cap where present. Finally, the 'code' column is set as the index for the DataFrame to uniquely identify each module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e26b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Data\n",
    "modules_facts['units'] = modules_facts['units'].map({'One Unit': 1.0, 'Half Unit': 0.5, 'Non-credit bearing': 0.0})\n",
    "modules_facts['total_students'] = modules_facts['total_students'].replace('Unavailable', np.nan).astype(float)\n",
    "modules_facts['avg_class_size'] = modules_facts['avg_class_size'].replace('Unavailable', np.nan).astype(float)\n",
    "modules_facts.loc[modules_facts['capped'].str.startswith('No'), 'capped'] = False\n",
    "modules_facts.loc[modules_facts['capped'] != False, 'capped'] = modules_facts.loc[modules_facts['capped'] != False, 'capped'].str.split(' ').str[1].str.strip('()').astype(int)\n",
    "modules_facts.set_index('code', inplace=True)\n",
    "modules_facts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d976f9",
   "metadata": {},
   "source": [
    "### 2.3.5. Mutually Exclusive Modules\n",
    "\n",
    "We are further interested in courses that are mutually exclusive to other courses, as we need to confirm valid course selection later on in our analysis. Thus, we manually created a CSV file containing all mutually exclusive course pairs, which we load into a dataframe, setting the 'code' column as the index for easier lookups. Printing out the dataframe info, confirms that there are 62 mutually exclusive pairs.\n",
    "\n",
    "*Note, that manual creation of this file was necessary due to major complications in the way information on mutual exclusive courses was stored in the course guides, with pairs often only mentioned in textblocks and their exclusivity explained with 'and' or 'or' statements, of which reliable data extraction would have superseeded our coding abilities.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a06e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutually_exclusive = pd.read_csv('data/modules/mutual_exclusive.csv')\n",
    "mutually_exclusive.set_index('code', inplace=True)\n",
    "mutually_exclusive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff74840",
   "metadata": {},
   "source": [
    "### 2.3.6. Merging Dataframes\n",
    "\n",
    "With our mutually exclusive data, we can now merge the two DataFrames by performing a left join based on the `code` column. This ensures that the mutually exclusive course data is added as a new column in the `modules_facts` DataFrame, with matching values for the corresponding courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mutually exclusive course data as a new column\n",
    "modules_facts = modules_facts.join(mutually_exclusive, how='left')\n",
    "\n",
    "# Display updated DataFrame\n",
    "modules_facts[modules_facts['mutually_exclusive_courses'].notna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1027350",
   "metadata": {},
   "source": [
    "### 2.3.7. Validating Cleaned Data\n",
    "\n",
    "To ensure data integrity before storage, we inspect the structure and completeness of the `modules_facts` DataFrame using `.info()`. The output shows that all modules have complete values for most columns, with missing values in `total_students` and `avg_class_size` due to unreported statistics in the course guides—these omissions are not critical for our analysis.\n",
    "\n",
    "All entries contain valid values in the prerequisites column, confirming that our scraper consistently extracted prerequisite information, even where no prerequisites were listed (resulting in empty lists).\n",
    "\n",
    "We also identify that only 56 out of the 62 mutually exclusive course pairs were successfully merged. By comparing the indices and manually inspecting the course guides, we confirm that the 6 missing course codes are absent from the course guides altogether, suggesting they correspond to outdated or discontinued modules. Since these are no longer offered, they are irrelevant for our current analysis and can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting rows containing NaN values\n",
    "print(modules_facts.info())\n",
    "\n",
    "# Identifying missing mutual exclusive pairs\n",
    "missing_codes = set(mutually_exclusive.index) - set(modules_facts.index)\n",
    "print('\\n'+\"Missing codes:\", missing_codes)\n",
    "print(f\"{len(missing_codes)} out of {len(mutually_exclusive)} total entries are missing.\")\n",
    "\n",
    "# Saving to CSV\n",
    "modules_facts.to_csv('data/modules/modules_key_facts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632c3e4",
   "metadata": {},
   "source": [
    "## 2.4. Outside Options\n",
    "\n",
    "Finally, we need to compile a list of standard outside options — elective modules available to most undergraduate students across various LSE programmes. While there is no centralized list explicitly outlining all shared outside options, manual inspection across different programme regulations (especially BSc Finance) reveals a consistent set of course codes frequently permitted as electives. We use the BSc Finance programme page to extract this list as a reliable proxy, which will be crucial for later analysis of elective pathways and degree flexibility.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aaca33",
   "metadata": {},
   "source": [
    "### 2.4.1. Identifying Outside Options List \n",
    "\n",
    "We begin by inspecting BSc Finance undergraduate programme page and parsing its HTML content with BeautifulSoup, identifying that this page contains a table listing the standard outside options allowed for most degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.lse.ac.uk/resources/calendar/programmeRegulations/undergraduate/2024/BScFinance.htm'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58f88e",
   "metadata": {},
   "source": [
    "### 2.4.2. Locating & Extracting Course Codes\n",
    "\n",
    "We iterate through all `<h3>` tags to locate the one linking to the Undergraduate Outside Options List, then identify the surrounding `<div>` and extract all course codes from the associated table. Upon inspection of the resulting dataframe we can count 374 elective modules that can be chosen outside a student’s main department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d065fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h3 in soup.find_all(\"h3\"):\n",
    "    a_tag = h3.find(\"a\")\n",
    "    if a_tag and \"Undergraduate Outside Options List\" in a_tag.text:\n",
    "        header = h3\n",
    "        break\n",
    "\n",
    "outside_course_codes = []\n",
    "\n",
    "parent_div = header.find_parent(\"div\")\n",
    "table = parent_div.find(\"table\")\n",
    "links = table.find_all(\"a\", href=True)\n",
    "for row in table.find_all(\"tr\"):\n",
    "    link = row.find(\"a\", href=True)\n",
    "    if link:\n",
    "        course_code = link.text.strip()\n",
    "        outside_course_codes.append(course_code)\n",
    "        \n",
    "outside_options = pd.DataFrame(outside_course_codes)\n",
    "outside_options.columns = ['code']\n",
    "outside_options.set_index('code', inplace=True)\n",
    "outside_options.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef5051f",
   "metadata": {},
   "source": [
    "### 2.4.3. Merging Dataframes\n",
    "\n",
    "We enrich the `outside_options` table by joining it with the `modules_facts` DataFrame to save a final dataset that solely contains all extra information on outside options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4412a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_options_df = outside_options.join(modules_facts, how='left')\n",
    "outside_options_df.to_csv('data/degrees/ug_outside_options.csv', index=False)\n",
    "outside_options_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af14032",
   "metadata": {},
   "source": [
    "## 2.5. Creating Complete Dataset\n",
    "\n",
    "To finalize our preprocessing, we now combine the course metadata and performance statistics into a single, unified dataset. This allows us to work with all relevant course-level variables — such as enrolment figures, difficulty metrics, prerequisites, and compatibility constraints — in a single table. We then apply this enriched dataset to the list of standard outside options (as defined in Section 1.4), enabling focused downstream analysis of student choice sets and performance characteristics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f9975",
   "metadata": {},
   "source": [
    "### 2.5.1. Merging Dataframes\n",
    "\n",
    "We perform a left join between `modules_facts` and `summary_df`, combining course characteristics (like class size, prerequisites, and mutually exclusive courses) with performance metrics (like mean marks, percentile scores, and pooled standard deviation). This yields a single comprehensive table indexed by course code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a1e48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_df = modules_facts.join(summary_df, how='left')\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58bb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_options_marks = outside_options.merge(summary_df, on='code', how='left')\n",
    "outside_options_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283b7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outside_options_marks = outside_options_marks.reset_index()\n",
    "outside_options_marks.sort_values(by='mean', ascending=False, inplace=True)\n",
    "outside_options_marks[[\"code\",'units_x','department','mean','q10','Median','q90','Pooled_SD','prerequisites','mutually_exclusive_courses']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f97a3",
   "metadata": {},
   "source": [
    "## 3.Degree Data Preparation and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dd05d-1ff1-4577-9c60-5a7201bf5dbf",
   "metadata": {},
   "source": [
    "### 3.1 Degree Data Exploration and Wrangling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079dd26-df47-4768-a7c0-18ceca5ab083",
   "metadata": {},
   "source": [
    "**3.1.1 Data Loading and Inspection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f62aae-06d1-4694-bda7-7c6e4ead02b7",
   "metadata": {},
   "source": [
    "We load in the CSV files with the data for programmes/degrees and individual modules and inspect the programme datafraframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a129917-9603-4611-a657-ccb2d97a17b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "programms_df = pd.read_csv('data//degrees/programme_data_NDC.csv')\n",
    "modules_df  = pd.read_csv('data//modules/modul_key_facts_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce944399-d415-4240-96e7-3aa99690c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "programms_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895938d-8293-4ecf-ba96-c655863ac5ee",
   "metadata": {},
   "source": [
    "**3.1.2 Dealing with missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959a217-5e19-4275-9f40-6162d52a820b",
   "metadata": {},
   "source": [
    "We inspect what values of the dataframe are missing replace them with 'None' in the case of a_level_extra values and drop the rows with no median salary recorded (we only drop according to median salary and not according to intake or nr_applications because we want to keep as much data as possible and null values are automaticaly not shown in the graphs whereas keeping null median salary values would affect out salary tables ). We identified 17 rows containing missing values, and hence have created a clean dataset using the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7fb4a-9deb-4902-b2f5-af63d8262a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "programms_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a47faa-5031-43ba-9d9c-cf114673889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "programms_df_clean = programms_df\n",
    "programms_df_clean['a_lvl_extra'] = programms_df_clean['a_lvl_extra'].fillna('None')\n",
    "print(f\"Missing values in 'a_lvl_extra' after replacement: {programms_df_clean['a_lvl_extra'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a3952-d108-46b9-a73a-aab7e3f2705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "programms_df_clean.dropna(subset=['median_salary'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97913f-b4c6-4655-a274-db25c2257b22",
   "metadata": {},
   "source": [
    "**3.1.3 Converting into numeric form and creating the acceptance percentage column**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6dd9b-3efa-434e-992d-f2d73ceaa08c",
   "metadata": {},
   "source": [
    "We clean the intake and nr_applications columns so they’re true numeric columns and we can do calcultions and then create a new column with the acceptance rate in decimal form rather than in ratio form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81a86f-a1e5-4ee9-b284-11d6de2e9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "programms_df_clean['intake'] = programms_df_clean['intake']\\\n",
    "    .astype(str)\\\n",
    "    .str.replace(r'[^\\d.]', '', regex=True)\n",
    "programms_df_clean['nr_applications'] = programms_df_clean['nr_applications']\\\n",
    "    .astype(str)\\\n",
    "    .str.replace(r'[^\\d.]', '', regex=True)\n",
    "\n",
    "programms_df_clean['intake'] = pd.to_numeric(programms_df_clean['intake'], errors='coerce')\n",
    "programms_df_clean['nr_applications'] = pd.to_numeric(programms_df_clean['nr_applications'], errors='coerce')\n",
    "programms_df_clean['acceptance_perc'] = (programms_df_clean['intake']/programms_df_clean['nr_applications']).round(5)\n",
    "programms_df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d09ae-1504-4a08-8c16-2c4e77ac3051",
   "metadata": {},
   "source": [
    "**3.1.4 Further converting to numeric form and tidying up the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e7aeb-c38a-467f-9106-ce4717873bcf",
   "metadata": {},
   "source": [
    "We convert the median salaries into numeric form and drop the columns we don't need anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed289b-f354-4ba4-8c47-672d70d10a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "programms_df_clean['median_salary'] = programms_df_clean['median_salary'].str.replace(',', '').str.replace('£', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbf05e-b6d1-4894-8b6a-751eed4cc682",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['nr_applications', 'intake', 'ratio', 'home_fee']\n",
    "programms_df_clean.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba3141-46bf-4c00-87bd-57ced54ee96c",
   "metadata": {},
   "source": [
    "**3.1.5 Second dataframe inspection and converting string cells into list cells**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4ca2a-bdee-435c-b8da-d3ac114076fc",
   "metadata": {},
   "source": [
    "Now we clean the three module columns by defining a fuction that converts each string into a list and apply it to the module columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af73fe2-07fe-4d70-8712-81710c5144aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_string(s):\n",
    "    if isinstance(s, str):\n",
    "        if pd.isna(s) or s.strip() == '[]': \n",
    "            return []\n",
    "        return [item.strip().strip(\"'\\\"\") for item in s.strip('[]').split(',')]\n",
    "    return [] \n",
    "for col in [\"modules_y1\", \"modules_y2\", \"modules_y3\"]:\n",
    "    programms_df_clean[col] = programms_df_clean[col].apply(parse_list_string)\n",
    "\n",
    "programms_df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e53128-1680-4c70-b5e4-731c23c26a8a",
   "metadata": {},
   "source": [
    "**3.1.6 Calculating the optionality of the degrees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eca4e0-0ebe-4bc5-9368-c919a77222ae",
   "metadata": {},
   "source": [
    "We now find the optionality (how many optimal modules there are) of each degree in terms of module units. We do that by defining a function that takes each compulsory module from the lists of each module column (the module columns only state the compulsory modules per year for each degree) and maps it to its corresponding unit value of either 0.5 or 1.0. After doing that we create 3 new columns which show how many units of compulsory modules each degree has for each year. We finally calculate the optionality by subtracting the total compulsory units per degree/programme, from the 12.5 units worth of modules that all undergrad degrees have at LSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16937fd0-6ef7-4ac8-a2ab-1ac7f8f3fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_units_dict = (modules_df.set_index(\"code\")[\"units\"].to_dict())\n",
    "def sum_units(cell, lookup):\n",
    " \n",
    "    if isinstance(cell, list):\n",
    "        return sum(lookup.get(m, 0) for m in cell)\n",
    "    return 0\n",
    "\n",
    "for year_col, total_col in [(\"modules_y1\", \"y1_total_units\"), (\"modules_y2\", \"y2_total_units\"), (\"modules_y3\", \"y3_total_units\"),]:\n",
    "    programms_df_clean[total_col] = programms_df_clean[year_col].apply(lambda cell: sum_units(cell, module_units_dict))\n",
    "\n",
    "programms_df_clean['total_compulsory_units'] = ( programms_df_clean['y1_total_units'] + programms_df_clean['y2_total_units'] + programms_df_clean['y3_total_units'])\n",
    "programms_df_clean['total_optional_units'] = 12.5 - programms_df_clean['total_compulsory_units']\n",
    "programms_df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0933e499-daed-40d7-97b1-d5e602c23b0e",
   "metadata": {},
   "source": [
    "### 3.2 Degree Data Analysis and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec96c588-4572-45be-90d6-86e193e56a4a",
   "metadata": {},
   "source": [
    "**3.2.1 Ranking courses based on optionality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281df858-c515-49b8-ae75-745484cdd6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_optional = (programms_df_clean[['degree', 'total_optional_units']].nlargest(5, 'total_optional_units').reset_index(drop=True))\n",
    "bot5_optional = (programms_df_clean[['degree', 'total_optional_units']].nsmallest(5, 'total_optional_units').reset_index(drop=True))\n",
    "pd.concat([top5_optional, bot5_optional], axis=1, keys=['Top 5 Optional Units', 'Bottom 5 Optional Units'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb799338-eefc-41fd-ac18-7e7c1025bafd",
   "metadata": {},
   "source": [
    "**3.2.2 Basic visualisation of salary distribution throughout degrees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20dfa4-7e7e-45bb-9216-d71d889038c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(x=programms_df_clean['median_salary'])\n",
    "plt.title('Distribution of Median Salaries for Degrees')\n",
    "plt.xlabel('Median Salary (£)')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6092f",
   "metadata": {},
   "source": [
    "The box plot above clearly depicts that the median 50% of programme graduation salaries are in the range of £35,000-£37,000. Finally, all programmes, apart from 3 outliers, lie within the range of £30,000 to £40,000, meaning that typically there is no great discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea370a-7b6f-4118-918f-38b6c18ed2ef",
   "metadata": {},
   "source": [
    "**3.2.3 Highest/lowest median salary degrees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c756c7b-5d22-49dc-8c30-c8f59cf90731",
   "metadata": {},
   "source": [
    "Now we look at specificaly which degrees have the highest and lowest median salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845fbf1-34b6-4671-9515-a64b21f5a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = programms_df_clean.nlargest(5, 'median_salary')[['degree', 'median_salary']]\\\n",
    "           .reset_index(drop=True)\n",
    "bottom5 = programms_df_clean.nsmallest(5, 'median_salary')[['degree', 'median_salary']]\\\n",
    "              .reset_index(drop=True)\n",
    "\n",
    "top5.columns = ['Top 5 Degrees', 'Top Salaries (£)']\n",
    "bottom5.columns = ['Bottom 5 Degrees', 'Bottom Salaries (£)']\n",
    "\n",
    "pd.concat([top5, bottom5], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e73c27-3682-45ca-b0e3-1743541da4c6",
   "metadata": {},
   "source": [
    "**3.2.4 Is there a relationship between A level requirements and median salaries?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8792303-c759-4227-9f03-0e41fca89fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirement_order = sorted(programms_df_clean['a_lvl_req'].unique(), reverse=True) \n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(data=programms_df_clean, x='a_lvl_req', y='median_salary', order=requirement_order, palette='coolwarm', hue='a_lvl_req')\n",
    "\n",
    "plt.title('Median Salary Distribution by A-Level Requirement')\n",
    "plt.xlabel('A-Level Requirement')\n",
    "plt.ylabel('Median Salary (£)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439710b8",
   "metadata": {},
   "source": [
    "This is one of the most informative graphs in our survey. Median salary seems to be heavily reliant on the A-level requirement of each degree. More specifically, each category of A-level requirement seems to be separated in margins. “AAB” ranges from £30,000 to £35,000; “AAA” ranges from £35,000 to £36,000; and “A*AA” ranges from £37,000 to £40,000. These observations refer to the median 50% of each category.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b0fd9-35b0-4000-a88b-aabeaf4a85eb",
   "metadata": {},
   "source": [
    "**3.2.5 What about the relationship between acceptance rates and median salaries?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a06b5-16ab-4dde-9dfc-8fd4f3366831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "sns.scatterplot(data=programms_df_clean, x='acceptance_perc', y='median_salary', hue='a_lvl_req', alpha=0.7)\n",
    "\n",
    "plt.title('Median Salary vs. Acceptance Percentage by Degree')\n",
    "plt.xlabel('Acceptance Percentage (Intake / Applications)')\n",
    "plt.ylabel('Median Salary (£)')\n",
    "plt.grid(linestyle='--', alpha=0.6)\n",
    "plt.legend(title='A-Level Req', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "correlation = programms_df_clean['acceptance_perc'].corr(programms_df_clean['median_salary'])\n",
    "print(f\"Correlation between Acceptance Percentage and Median Salary: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4a3af-0cc8-490b-8cff-772fc47e9c1a",
   "metadata": {},
   "source": [
    " It can be seen that there is basically no significant trend between acceptance rates and median salaries and that A*AA degrees are above all other degrees with lower grade requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a33b7",
   "metadata": {},
   "source": [
    "## 4. Module Data Preperation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701b777",
   "metadata": {},
   "source": [
    "* 4.1. _Preparing Module Data Tables_\n",
    "* 4.2.\t_Why did we choose these degrees_\n",
    "* 4.3.\t_The general method employed_\n",
    "* 4.4.\t_Discovering the Designs_\n",
    "   * 4.41. Finance\n",
    "   * 4.42. Economics\n",
    "   * 4.43. International Relations\n",
    "   * 4.44. Politics and Economics\n",
    "   * 4.45. Physchological and Behavioural Sciences\n",
    "* 4.5 _Putting it All Together_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aaa2c9",
   "metadata": {},
   "source": [
    "###  4.1 Preparing Module Table and Outside Option Table\n",
    "\n",
    "*We prepare tables we need for preparing the degree designs, and also begin preparing tables giving us a sense of mark distribution\n",
    "\n",
    "**4.1.1 Creating Merged Tables** \n",
    "* We begin by grouping up the different course code to isolate the data for the summary statistics for each course code. We took the arithmetic mean across the different years for each course code. Finally we duplicated this table and merged with the outside option list we scraped from before to create a table of outside options and their respective scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59eeba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marks_df = pd.read_csv('data/modules/marks_summary_modules.csv')\n",
    "\n",
    "# We only looked at courses which existed in 23.24\n",
    "marks_df = marks_df[marks_df['code'].isin(marks_df[marks_df['year'] == '2023/24']['code'].unique())]\n",
    "\n",
    "\n",
    "summary_df = marks_df.groupby('code').apply(\n",
    "    lambda group: pd.Series({\n",
    "        'mean': np.average(group['mean'], weights=group['marks']) if group['marks'].sum() > 0 else np.nan,\n",
    "        'q10': group['q10'].mean() if group['marks'].sum() > 0 else np.nan,\n",
    "        'Median': group['median'].mean() if group['marks'].sum() > 0 else np.nan,\n",
    "        'q90': group['q90'].mean() if group['marks'].sum() > 0 else np.nan,\n",
    "        'Pooled_SD': (\n",
    "            np.sqrt(((group['marks'] - 1) * group['sd'] ** 2).sum() / (group['marks'].sum() - len(group)))\n",
    "            if group['marks'].sum() - len(group) > 0 else np.nan),\n",
    "        'Department': group['department'].mode()[0] if not group['department'].mode().empty else np.nan\n",
    "    })).reset_index()\n",
    "\n",
    "# Merging the DataFrame We Made above with another dataframe to add the units for each course code\n",
    "module_facts_df = pd.read_csv(\"data/modules/modul_key_facts_updated.csv\")\n",
    "summary_df = summary_df.merge(module_facts_df[['code', 'units']], on='code', how='left')\n",
    "\n",
    "# Getting the Marks Distribution of Outside Options\n",
    "outside_options = pd.read_csv('data/modules/outside_options.csv')\n",
    "outside_options_marks = outside_options.merge(summary_df,left_on='code',right_on='code',how='left')\n",
    "outside_options_marks.drop(columns=['course', 'total_students', 'avg_class_size', 'capped'], inplace=True)\n",
    "outside_options_marks.sort_values(by='mean', ascending=False, inplace=True)\n",
    "outside_options_marks[[\"code\",'units_x','department','mean','q10','Median','q90','Pooled_SD','prerequisites','mutually_exclusive_courses']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62743aca",
   "metadata": {},
   "source": [
    "**4.1.2 Calculating the Mean of The Compulsory Modules**\n",
    "\n",
    "* We began by defining a very crucial function, which works out the arithmetic mean of all the summary statistics for each of the course codes in any specified list of modules. We then apply this to the list of compulsory modules to test if it works as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82708452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics_for_modules(module_list, summary_df):\n",
    "    selected_modules = summary_df[summary_df['code'].isin(module_list)]\n",
    "    weights = selected_modules['units']\n",
    "    total_weight = weights.sum()\n",
    "\n",
    "    mean_value = (selected_modules['mean'] * weights).sum() / total_weight\n",
    "    q10_value = (selected_modules['q10'] * weights).sum() / total_weight\n",
    "    median_value = (selected_modules['Median'] * weights).sum() / total_weight\n",
    "    q90_value = (selected_modules['q90'] * weights).sum() / total_weight\n",
    "    pooled_sd_value = (selected_modules['Pooled_SD'] * weights).sum() / total_weight\n",
    "\n",
    "    return mean_value, q10_value, median_value, q90_value, pooled_sd_value\n",
    "\n",
    "programms_df_clean['modules'] = programms_df_clean['modules_y1'] +programms_df_clean['modules_y2']\n",
    "+programms_df_clean['modules_y3']\n",
    "\n",
    "programms_df_clean[['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']] = programms_df_clean['modules'].apply(\n",
    "    lambda x: pd.Series(\n",
    "        calculate_statistics_for_modules(x, summary_df)\n",
    "        if calculate_statistics_for_modules(x, summary_df) is not None\n",
    "        else [np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        index=['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']\n",
    "    ))\n",
    "\n",
    "programms_df_clean = programms_df_clean.sort_values(by='Mean', ascending=False)\n",
    "programms_df_clean[[\"degree\",\"Mean\",\"Median\",\"Q10\",\"Q90\",\"Pooled_SD\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a5bb2",
   "metadata": {},
   "source": [
    "**4.1.3 First Year Outside Options** \n",
    "* We now filter outside options down to only include 1st year courses in a dataframe we call first year outside options. This is because certain courses have their 1st years choose from a list of 1st year outside options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f526b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_options_1st_year = outside_options_marks.copy()\n",
    "outside_options_1st_year['course year'] = outside_options_1st_year['code'].str[2].astype(float)\n",
    "outside_options_1st_year = outside_options_1st_year[outside_options_1st_year['course year'] == 1.0]\n",
    "outside_options_1st_year = outside_options_1st_year.dropna(subset=['mean'])\n",
    "outside_options_1st_year[[\"code\",'units_x','department','mean','q10','Median','q90','Pooled_SD','prerequisites','mutually_exclusive_courses']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68903ee5",
   "metadata": {},
   "source": [
    "**4.1.4 Module Score Distribution by Department**\n",
    "* We create a table which groups up modules by department, taking an average of the means and providing a standard deviation figure for the courses in the department. We do this to really understand the distribution of scores between modules in order to spot any trends that may be useful down the line in preparing each degree design. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7457b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "department_dist = summary_df.groupby('Department').agg(\n",
    "    Department_Mean=('mean', 'mean'),\n",
    "    Department_SD=('mean', 'std')\n",
    ").reset_index()\n",
    "department_dist_sorted = department_dist.sort_values(by='Department_SD', ascending=False).reset_index(drop=True)\n",
    "department_dist_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d1e9b",
   "metadata": {},
   "source": [
    "### 4.2 Why did we choose these degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ab5ce",
   "metadata": {},
   "source": [
    "**4.2.1**   \n",
    "   * Upon inspection of the different course pages, we quickly discovered how different the pages and courses were from each other. Making one master code which could analyse every single degree to produce a comprenhesive analysis would prove very difficult and prone to error. We hence limited our analysis to 5 degrees, all with certain charateristics which we believe make them interesting degrees to study. By limiting the number of degrees we could really ensure accuracy and that the choices of module that a student can make is very realistic. \n",
    "   * Due to every course being different from each other, we try defining functions which are as flexible as possible. This proved to work well. Whenever this broke down however we varied the code to handle these differences for each degree\n",
    "   * We define an easy degree design as one which has the lowest mean-of means of the modules the student takes, a hard degree design is the highest mean of means of modules the student takes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949ef0f",
   "metadata": {},
   "source": [
    "**4.2.2 The Degrees**\n",
    "\n",
    "* _Finance_  - Only 2 units of Optionality (Least at LSE)\n",
    "* _Economics_ - High Graduation Salary, Very Low Acceptance and one of LSE's most Prestigous Courses\n",
    "* _International Relations_ - 11 Units of Optionality, and lowest Graduation Salary\n",
    "* _Politics and Economics_ - Most units chosen from the outside options list, and a double award degree\n",
    "* _Physchological and Behavioural Science_ - highest average amongst it's compulsory modules\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa391b4",
   "metadata": {},
   "source": [
    "### 4.3 General Method Employed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a37b23",
   "metadata": {},
   "source": [
    " * Whilst we have scraped all the compulsory courses for each degree possible, we discovered that in some cases there were errors in the list due to the scraping logic not being sophistaced enough to handle the very large differences in course pages. The optional courses were another issue entirely and so we manually created csv files for the degrees we chose to analyse, with the number of units, if its optional/compulsory and the year the module can be taken\n",
    "\n",
    "* We then prepare each possible combination of module that the student can take and seperate the student into a 'path' based on their 1st year optional module choice.We then use different types of logic, aided by a manual check of the data to ensure compatability to ensure all the optional modules comply with that degree design  of the hardest, easiest and middle values for the mean of means.\n",
    "\n",
    "* We then add to this one big table with all the degrees to use for further analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e6234",
   "metadata": {},
   "source": [
    "### 4.4 Preparing the Degree Designs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8556a",
   "metadata": {},
   "source": [
    "### 4.41 Finance\n",
    "\n",
    "We begin with Finance (our degree), where we look and define most of the functions we will use for the rest of the degrees. \n",
    "\n",
    "**4.41.1: Hypothesis**\n",
    "* As Finance has very little optionality, we hypothesise that the difference between an easy and a hard modules would be limited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de626593",
   "metadata": {},
   "source": [
    "**4.41.2: Loading in the Finance Module Options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe399edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_options = pd.read_csv(\"data/degrees/financem.csv\")\n",
    "finance_options.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a63e7",
   "metadata": {},
   "source": [
    "**4.41.2 Preparing a table with just compulsory modules** \n",
    "* We now define a function which looks at all the compulsory modules a student can take ,and create a dataframe which makes a list of modules, the number of units taken and how many more units the student needs to take in first year (that isn't an outside option), marked OO in the Finance Options dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_compulsory_modules(df, standard_units_per_year=4):\n",
    "    df['Units'] = pd.to_numeric(df['Units'])\n",
    "    df['Year'] = pd.to_numeric(df['Year'])\n",
    "    \n",
    "    comp = df[df['Optional/Compulsory'] == 0]\n",
    "    oo = df[df['Module'].str.upper().str.contains('OO')]\n",
    "\n",
    "    y1_comp = comp[comp['Year'] == 1]['Units'].sum()\n",
    "    y1_oo = oo[oo['Year'] == 1]['Units'].sum()\n",
    "    y1_non_oo_needed = standard_units_per_year - y1_comp - y1_oo\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        'Compulsory Modules': comp['Module'].tolist(),'Total Compulsory Units': comp['Units'].sum(),\n",
    "        'Year 1 Non-OO Units Needed': y1_non_oo_needed}])\n",
    "\n",
    "initial_combinations_dffi = summarize_compulsory_modules(finance_options)\n",
    "initial_combinations_dffi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a57db8",
   "metadata": {},
   "source": [
    "**4.41.3 Creating 'Paths' for each of the 1st-year optionals** \n",
    "* We define a function, which looks for all of the optional modules that aren't an outside option in 1st year to then create the list of modules which are compulsory and then the 1st year optional module the student chose. We decided to segment it like this, because 1st year choices are made before a student properly joins LSE and hence they wouldn't take course difficulty into account as much as their interest in learning that module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def generate_year1_optional_combinations(base_summary_df, all_modules_df, standard_units_per_year=4):\n",
    "    needed = base_summary_df.iloc[0]['Year 1 Non-OO Units Needed']\n",
    "    comp_modules = base_summary_df.iloc[0]['Compulsory Modules']\n",
    "    comp_units = base_summary_df.iloc[0]['Total Compulsory Units']\n",
    "\n",
    "    modules = all_modules_df[\n",
    "        (all_modules_df['Optional/Compulsory'] == 1) & (all_modules_df['Year'] == 1) &  (~all_modules_df['Module'].str.upper().str.contains('OO'))\n",
    "        ][['Module', 'Units']].dropna().values.tolist()\n",
    "    \n",
    "    if not modules:\n",
    "        return base_summary_df\n",
    "    \n",
    "    rows = []\n",
    "    for r in range(1, len(modules) + 1):\n",
    "        for combo in combinations(modules, r):\n",
    "            if sum(u for _, u in combo) == needed:\n",
    "                opts = [m for m, _ in combo]\n",
    "                rows.append({\n",
    "                    'Modules': comp_modules + opts, 'Optional Modules Added': opts, 'Total Units': comp_units + needed})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "year1_combos_dffi = generate_year1_optional_combinations(initial_combinations_dffi, finance_options)\n",
    "year1_combos_dffi2 =year1_combos_dffi\n",
    "year1_combos_dffi2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5e8b2",
   "metadata": {},
   "source": [
    "**4.41.4 Creating subrows** \n",
    "* This function just manipulates the dataframe, so that each optional module choice has a difficulty subrow as is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_difficulty_subrows(year1_combos_dfirf):\n",
    "    subrows = []\n",
    "    \n",
    "    for _, row in year1_combos_dfirf.iterrows():\n",
    "        for difficulty in ['Hard', 'Medium', 'Easy']:\n",
    "            subrow = row.copy()\n",
    "            subrow['Difficulty'] = difficulty\n",
    "            subrows.append(subrow)\n",
    "    \n",
    "    year1_combos_dfirf_expanded = pd.DataFrame(subrows)\n",
    "    \n",
    "    return year1_combos_dfirf_expanded\n",
    "year_1_combos_dffi3 = create_difficulty_subrows(year1_combos_dffi2)\n",
    "year_1_combos_dffi4=year_1_combos_dffi3.copy()\n",
    "year_1_combos_dffi4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf978f",
   "metadata": {},
   "source": [
    "**4.41.5 Determining the Outside Options the Student Takes**\n",
    "\n",
    "* We define a function which searches a dataframe list of modules, and attempts to calculate the combination of modules with highest, middle and lowest mean values for a specified number of units. We will then manually check to see if that combination actually works against prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_of_means_combinationsOO(df, target_units=4.0, code_col='code', mean_col='mean', unit_col='units_x'):\n",
    "    valid_combos = [\n",
    "        (\", \".join(getattr(c, code_col) for c in combo), np.mean([getattr(c, mean_col) for c in combo]))\n",
    "        for r in range(1, len(df) + 1)\n",
    "        for combo in itertools.combinations(df.itertuples(index=False), r)\n",
    "        if np.isclose(sum(getattr(c, unit_col) for c in combo), target_units)]\n",
    "\n",
    "    sorted_combos = sorted(valid_combos, key=lambda x: x[1])\n",
    "    mid_index = len(sorted_combos) // 2\n",
    "\n",
    "    \n",
    "    print(\"Lowest Mean of Means Combo:\", sorted_combos[0])\n",
    "    print(\"Median Mean of Means Combo:\", sorted_combos[mid_index])\n",
    "    print(\" Highest Mean of Means Combo:\", sorted_combos[-1])\n",
    "\n",
    "\n",
    "#find_mean_of_means_combinationsOO(outside_options_marks, target_units=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965e102f",
   "metadata": {},
   "source": [
    "**4.41.6 Big Issue** \n",
    "* The code above produces a combinations of outside options up to 1.5 units but this took our computers took over an hour to run the code. This was because the number of combinations was in the billions. We realized that the process for finding outside options was going to have to go down a different front for outside options. We can only use this on lists of optional courses where the number of courses is much smaller\n",
    "* The original plan would be to look at the prerequisites list for each of the outside options the student can take, and then from there work out if that outside option is available to be taken based on the students prior modules. However the section listing prerequisites is incredibly different for every single module and it was impossible for us to come up with a way to extract all information effectively. We settled for adding it as a prerequsite if the course code is mentioned in the prerequisite subheading. However a lot of the prerequisites are mixed with Ands, Or's, or 2 of the 4, and it was very tricky for us to integrate this accurately. So we settled for simply analysing the list of outside options based on mean and choosing modules which are definitely valid and a realistic choice. This was time consuming and removed the automation of our code but we believe we preserved accuracy and hence this was a trade-off we wanted to make.  Furthermore, some outside options have other outside options as prerequisites and the code would then have to be dynamic which would significantly increase the length of code needed. So we defined a function which sorts the means of the outside options mark dataframe and displays a custom number of hard, easy and median mean value modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bddfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sorted_by_mean(dataframe, num=5):\n",
    "    df = dataframe.dropna(subset=['mean']).sort_values(by='mean')\n",
    "    print('Hardest'); display(df.iloc[:num][['code', 'mean', 'prerequisites', 'mutually_exclusive_courses','units_x']])\n",
    "    print('Median'); display(df.iloc[len(df)//2 - num//2 : len(df)//2 + num//2 + (num%2)][['code', 'mean', 'prerequisites', 'mutually_exclusive_courses','units_x']])\n",
    "    print('Easiest'); display(df.iloc[-num:][['code', 'mean', 'prerequisites', 'mutually_exclusive_courses','units_x']])\n",
    "\n",
    "\n",
    "print_sorted_by_mean(outside_options_marks,num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b9604",
   "metadata": {},
   "source": [
    "**4.41.7 Outside Option Selection**\n",
    "\n",
    "* Upon inspection of the dataframes above, we determined the hardest, easiest and middle outcomes of 1.5 units worth of outside options. We then manually checked this to ensure they were completely valid and then defined a function which would aim to add the dataframe to the one we have already produced above\n",
    "* We look at the median outcome as well throughout the analysis of degrees to understand what someone who wouldn't pay attention to module difficulty would pick on average. This was done as sanity check of our data to udnerstand the distribution within each degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba242ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_with_difficulty_variants(year1_combos_df, difficulty_df):\n",
    "    rows = []\n",
    "\n",
    "    for _, combo in year1_combos_df.iterrows():\n",
    "        for _, diff in difficulty_df.iterrows():\n",
    "            if (combo['Difficulty'] == diff['Difficulty'] and \n",
    "                combo['Optional Modules Added'] == diff['Optional Modules Added']):\n",
    "                rows.append({\n",
    "                    'Modules': combo['Modules'] + diff['OO_added'],\n",
    "                    'Optional Modules Added': combo['Optional Modules Added'] + diff['OO_added'],\n",
    "                    'Difficulty': combo['Difficulty'],\n",
    "                    **{k: combo[k] for k in combo.index if k not in ['Modules', 'Optional Modules Added', 'Difficulty']}\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ed278",
   "metadata": {},
   "source": [
    "Based on the list of outside options and the students intitial choices, these are the valid outside options the student would take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473db8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty_dffi = pd.DataFrame([\n",
    "    {'Difficulty': 'Hard','Optional Modules Added': ['AC102'],'OO_added': ['MA102', 'AC332', 'EC339']},\n",
    "    {'Difficulty': 'Medium', 'Optional Modules Added': ['AC102'], 'OO_added': [ 'AC342', 'LL251']}, \n",
    "    {'Difficulty': 'Easy',   'Optional Modules Added': ['AC102'], 'OO_added': ['DS105A', 'EH209']},\n",
    "    {'Difficulty': 'Hard', 'Optional Modules Added': ['ST101'],'OO_added':[ 'MA102', 'LL106']  }, \n",
    "    {'Difficulty': 'Medium',  'Optional Modules Added': ['ST101'],  'OO_added': ['AN237', 'LL251'] },\n",
    "    {'Difficulty': 'Easy', 'Optional Modules Added': ['ST101'], 'OO_added': ['DS105A', 'ST310', 'ST311'] }\n",
    "])\n",
    "\n",
    "finance_combos = expand_with_difficulty_variants(year_1_combos_dffi4, difficulty_dffi)\n",
    "finance_combos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32321c0",
   "metadata": {},
   "source": [
    "**4.41.8 Calculating Statistics for the Modules Chosen**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe09e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_combos[['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']] = finance_combos['Modules'].apply(\n",
    "    lambda x: pd.Series(calculate_statistics_for_modules(x, summary_df))\n",
    ")\n",
    "finance_combos[\"degree\"] = \"BSc Finance\"\n",
    "finance_combos[[\"Modules\",\"Difficulty\",\"Mean\",\"Median\",\"Q10\",\"Q90\",\"Pooled_SD\",\"degree\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36c165",
   "metadata": {},
   "source": [
    "**4.41.9 The Finance Table**\n",
    "\n",
    "* We ended up with this table which lists the modules, and the means based on the difficulty the student can choose. If any of the modules do not have any data points, then as they are treated as NaN modules the mean calculation skips them over. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399a8d3",
   "metadata": {},
   "source": [
    "There are a few interesting obsevrations that can be made here:\n",
    "\n",
    "* The easiest possible combination for Finance is very similar to the recommended modules that the department suggests to their students.\n",
    "* It does not appear that there is much of a difference between hard and easy degree designs. We suspect this is because there is very little choice a student can make and hence even if there is a big difference in easy and hard modules it takes up a small % of the overall degree\n",
    "* We conduct further analysis in the analysis section\n",
    "\n",
    "Finally, our hypothesis appears to be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ddc94",
   "metadata": {},
   "source": [
    "### 4.42 Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790d10e",
   "metadata": {},
   "source": [
    "**4.42.1 Hypothesis** \n",
    "We now analyse Economics, using a lot of the logic we defined above, and a few new functions which fit into the course structure that Economics has. Economics has a high graduation salary, a median amount of optionality and a low acceptance rate. Our hypothesis is that is quite unclear what the difference between easy and hard will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f023ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_options = pd.read_csv(\"data/degrees/econm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a037db",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_combinations_df3 = summarize_compulsory_modules(econ_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb31837",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_combos_df = generate_year1_optional_combinations(initial_combinations_df3, econ_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bfee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_1_combos_df2 = create_difficulty_subrows(year1_combos_df)\n",
    "year_1_combos_df3=year_1_combos_df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8e112",
   "metadata": {},
   "source": [
    "**4.42.2**\n",
    "* Economics has an outside options choice in its 1st year. This list is difference to the other outside options list so we pick a module from there for the different designs of the degree.We used the print sorted mean to discover the 1st year outside options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42509162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_sorted_by_mean(outside_options_1st_year,num=5)\n",
    "#This hasn't been shown as the output is very similar to when it was run earlier, and throughout the rest of the report, will never be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7088b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty_df = pd.DataFrame([\n",
    "    {'Difficulty': 'Hard','Optional Modules Added': ['MA100'],'OO_added': ['MA102',\"ST101A\"]}, \n",
    "    {'Difficulty': 'Medium', 'Optional Modules Added': ['MA100'], 'OO_added': ['PH103']}, \n",
    "    {'Difficulty': 'Easy', 'Optional Modules Added': ['MA100'],'OO_added': ['DS105A', 'AC102'] }, \n",
    "    {'Difficulty': 'Hard', 'Optional Modules Added': ['MA108', 'EC1B1'],'OO_added': ['MA102',\"ST101A\"] },\n",
    "    {'Difficulty': 'Medium',  'Optional Modules Added': ['MA108', 'EC1B1'],'OO_added': ['PH103'] },\n",
    "    {'Difficulty': 'Easy', 'Optional Modules Added': ['MA108', 'EC1B1'], 'OO_added': ['DS105A', 'AC102'] }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70eb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_1incOO_df = expand_with_difficulty_variants(year_1_combos_df3, difficulty_df)\n",
    "year_1incOO_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a710b",
   "metadata": {},
   "source": [
    "**4.42.2 Completing Optional Course Info**\n",
    "* we now create code which adds to the options in the course options dataframe, including a lot of the information around means. We then create a table which only looks at the 3rd Year optional courses in Economics (that aren't OO) or courses that dont have any data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d1c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_summary_and_prereqs(econ_options, summary_df, outside_options_marks):\n",
    "    merged_df = pd.merge(econ_options, summary_df, left_on='Module', right_on='code', how='left')\n",
    "    merged_with_outside = pd.merge(merged_df, outside_options_marks[['code', 'prerequisites', 'mutually_exclusive_courses']],\n",
    "                                   left_on='Module', right_on='code', how='left')\n",
    "\n",
    "    if 'code' in merged_with_outside.columns:\n",
    "        merged_with_outside = merged_with_outside.drop(columns=['code'])\n",
    "    \n",
    "    return merged_with_outside\n",
    "\n",
    "merged_df_e = add_summary_and_prereqs(econ_options, summary_df,outside_options_marks)\n",
    "\n",
    "merged_df_e3= merged_df_e[merged_df_e['Year'] == 3]\n",
    "merged_df_e3 = merged_df_e3.sort_values(by='mean', ascending=False)\n",
    "merged_df_e3 = merged_df_e3.dropna(subset=['mean'])\n",
    "merged_df_e3 = merged_df_e3[~merged_df_e3['Module'].isin(['FM322', 'FM321'])]\n",
    "merged_df_e3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_of_means_combinations(df, target_units=4.0, code_col='Module', mean_col='mean', unit_col='Units'):\n",
    "    valid_combos = [\n",
    "        (\", \".join(getattr(c, code_col) for c in combo), np.mean([getattr(c, mean_col) for c in combo]))\n",
    "        for r in range(1, len(df) + 1)\n",
    "        for combo in itertools.combinations(df.itertuples(index=False), r)\n",
    "        if np.isclose(sum(getattr(c, unit_col) for c in combo), target_units)\n",
    "    ]\n",
    "\n",
    "    sorted_combos = sorted(valid_combos, key=lambda x: x[1])\n",
    "    mid_index = len(sorted_combos) // 2\n",
    "\n",
    "    \n",
    "    print(\"Lowest Mean of Means Combo:\", sorted_combos[0])\n",
    "    print(\"Median Mean of Means Combo:\", sorted_combos[mid_index])\n",
    "    print(\" Highest Mean of Means Combo:\", sorted_combos[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4c2a4",
   "metadata": {},
   "source": [
    "**4.42.3 Optional Courses Combinations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e50795",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_mean_of_means_combinations(merged_df_e3, target_units=4.0)\n",
    "#print_sorted_by_mean(outside_options_marks,num=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty_df2 = pd.DataFrame([\n",
    "    {'Difficulty': 'Hard','Optional Modules Added': ['MA100','MA102',\"ST101A\"],'OO_added': ['EC302', 'EC310','EC337','EC319','EC335','MA212']}, \n",
    "    {'Difficulty': 'Medium', 'Optional Modules Added': ['MA100','PH103'], 'OO_added': ['EC338', 'EC313', 'EC321', 'EC311', 'EC339','HY206']}, \n",
    "    {'Difficulty': 'Easy', 'Optional Modules Added': ['MA100','DS105A', 'AC102'],'OO_added': ['EC336', 'EC338', 'EC307', 'PH311', 'EC330','LN200'] }, \n",
    "    {'Difficulty': 'Hard', 'Optional Modules Added': ['MA108', 'EC1B1','MA102',\"ST101A\"],'OO_added': ['EC302', 'EC310','EC337','EC319','EC335','MA212'] },\n",
    "    {'Difficulty': 'Medium',  'Optional Modules Added': ['MA108', 'EC1B1','PH103'],'OO_added': ['EC338', 'EC313', 'EC321', 'EC311', 'EC339','HY206'] },\n",
    "    {'Difficulty': 'Easy', 'Optional Modules Added': ['MA108', 'EC1B1','DS105A','AC102'], 'OO_added': ['EC336', 'EC338', 'EC307', 'PH311', 'EC330','LN200'] }\n",
    "])\n",
    "\n",
    "year_1incOO_df2 = expand_with_difficulty_variants(year_1incOO_df, difficulty_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_1incOO_df2[['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']] = year_1incOO_df2['Modules'].apply(\n",
    "    lambda x: pd.Series(\n",
    "        calculate_statistics_for_modules(x, summary_df)\n",
    "        if calculate_statistics_for_modules(x, summary_df) is not None\n",
    "        else [np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        index=['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']\n",
    "    )\n",
    ")\n",
    "year_1incOO_df2[\"degree\"] = \"BSc Economics\"\n",
    "year_1incOO_df2[[\"Optional Modules Added\",\"Difficulty\",\"Mean\",\"Median\",\"Q10\",\"Q90\",\"Pooled_SD\",\"degree\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ea0ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c98bb0d",
   "metadata": {},
   "source": [
    "**4.42.4 Final Table** \n",
    "* On quick inspection there is a much wider difference between easy and hard module here. Potentially because there is a lot more choice. Our hypothesis doesn't really hold here. We suspect this is because there is a lot of hard and easy economics modules. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8413196",
   "metadata": {},
   "source": [
    "### 4.43 International Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67388ad4",
   "metadata": {},
   "source": [
    "**4.43.1 Hypothesis**\n",
    "* International Relations is a degree with a lot of choice, and no outside options and hence we thought it is an interesting degree to analyse. We predict a large range here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeae111",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_options = pd.read_csv(\"data/degrees/irm.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_combinations_dfir = summarize_compulsory_modules(ir_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3259e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_combos_dfir = generate_year1_optional_combinations(initial_combinations_dfir, ir_options)\n",
    "year_1_combos_dfir2 = create_difficulty_subrows(year1_combos_dfir)                                  \n",
    "year_1_combos_dfir2                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d84f3",
   "metadata": {},
   "source": [
    "**4.43.2** \n",
    "* We manualy remove all the mutually exclusive combinations of 1st year options in IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_combos_dfirf2 = year_1_combos_dfir2[year_1_combos_dfir2['Optional Modules Added'].isin([[ 'HY113', 'HY116' ], [ 'HY113', 'EH101' ],\n",
    "[ 'HY113', 'GV101' ],[ 'HY113', 'PH103' ],[ 'HY113', 'SO100' ],[ 'HY116', 'EH101' ],[ 'HY116', 'GV101' ],[ 'HY113', 'PH103' ],\n",
    "['HY113', 'SO10O' ]])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_ir = add_summary_and_prereqs(ir_options, summary_df,outside_options_marks)\n",
    "\n",
    "merged_df_ir3= merged_df_ir[merged_df_ir['Year'] == 3]\n",
    "merged_df_ir3 = merged_df_ir3.sort_values(by='mean', ascending=False)\n",
    "merged_df_ir3 = merged_df_ir3.dropna(subset=['mean'])\n",
    "merged_df_ir3 = merged_df_ir3.dropna(subset=['mean'])\n",
    "merged_df_ir3 = merged_df_ir3[~merged_df_ir3['Module'].isin(['LL342'])]\n",
    "\n",
    "merged_df_ir2= merged_df_ir[merged_df_ir['Year'] == 2]\n",
    "merged_df_ir2 = merged_df_ir2.sort_values(by='mean', ascending=False)\n",
    "merged_df_ir2 = merged_df_ir2.dropna(subset=['mean'])\n",
    "\n",
    "result_ir2 = find_mean_of_means_combinations(merged_df_ir2, target_units=4.0)\n",
    "result_ir3 = find_mean_of_means_combinations(merged_df_ir3, target_units=3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_dict_ir3 = {\n",
    "    'Hard': ['IR315','IR380','IR317','IR379','IR368','IR206','IR203','IR205','IR202'],\n",
    "    'Medium': ['IR373','IR395','IR398','IR317','IR379','IR200','IR206','IR205','IR202'],\n",
    "    'Easy': ['IR377' ,'IR373','IR391','IR323','IR398','IR200','IR206','IR203','IR205']\n",
    "}\n",
    "\n",
    "year1_combos_dfirf2.loc[:, 'Modules'] = year1_combos_dfirf2.apply(\n",
    "    lambda row: list(set(row['Modules'] + append_dict_ir3.get(row['Difficulty'], []))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "year1_combos_dfirf2.loc[:, 'Optional Modules Added'] = year1_combos_dfirf2['Difficulty'].apply(\n",
    "    lambda diff: append_dict_ir3.get(diff, [])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_combos_dfirf2 = year1_combos_dfirf2.copy() \n",
    "year1_combos_dfirf2[['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']] = year1_combos_dfirf2['Modules'].apply(\n",
    "    lambda x: pd.Series(\n",
    "        calculate_statistics_for_modules(x, summary_df)\n",
    "        if calculate_statistics_for_modules(x, summary_df) is not None\n",
    "        else [np.nan] * 5,\n",
    "        index=['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']\n",
    "    )\n",
    ")\n",
    "\n",
    "year1_combos_dfirf2[\"degree\"] = \"BSc International Relations\"\n",
    "year1_combos_dfirf2[[\"Optional Modules Added\", \"Difficulty\", \"Mean\", \"Median\", \"Q10\", \"Q90\", \"Pooled_SD\", \"degree\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461ca63",
   "metadata": {},
   "source": [
    "**4.43.3 Final Table**\n",
    "* Loads of options, but not much of a difference! We suspect this is because even though there is a lot of choice in terms of the absolute number, the degree is choosing from fixed lists which are not much longer than the number of units required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5da972",
   "metadata": {},
   "source": [
    "### Politics and Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2994b6",
   "metadata": {},
   "source": [
    "**4.44.1 Hypothesis**\n",
    "* Particularly after what we found above, we looked for the degree which has the most outside options (there were a lot of ties), so we chose Politics and Economics because it is a dual degree and hence theoretically has access to quite a few different option types. We hypothesise a wide range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9074f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_options = pd.read_csv(\"data/degrees/pem.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa400bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_combinations_dfpe = summarize_compulsory_modules(pe_options)\n",
    "initial_combinations_dfpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_combos_dfpe = generate_year1_optional_combinations(initial_combinations_dfpe, pe_options)\n",
    "year1_combos_dfpe2 = create_difficulty_subrows(year1_combos_dfpe)\n",
    "year1_combos_dfpe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88968c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_pe = add_summary_and_prereqs(pe_options, summary_df,outside_options_marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ee4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_pe2= merged_df_pe[merged_df_pe['Year'] == 2]\n",
    "merged_df_pe2 = merged_df_pe2.sort_values(by='mean', ascending=False)\n",
    "merged_df_pe2 = merged_df_pe2.dropna(subset=['mean'])\n",
    "merged_df_pe2 = merged_df_pe2[merged_df_pe2['Optional/Compulsory'] == 1.0]\n",
    "\n",
    "\n",
    "\n",
    "merged_df_pe3GV =merged_df_pe[merged_df_pe['Year'] == 3]\n",
    "merged_df_pe3GV = merged_df_pe3GV[merged_df_pe3GV['Optional/Compulsory'] == 1.0]\n",
    "merged_df_pe3GV = merged_df_pe3GV.sort_values(by='mean', ascending=False)\n",
    "merged_df_pe3GV = merged_df_pe3GV.dropna(subset=['mean'])\n",
    "merged_df_pe3GV = merged_df_pe3GV[merged_df_pe3GV['Module'].str.startswith('GV')]\n",
    "\n",
    "\n",
    "\n",
    "merged_df_pe3EC =merged_df_pe[merged_df_pe['Year'] == 3]\n",
    "merged_df_pe3EC = merged_df_pe3EC[merged_df_pe3EC['Optional/Compulsory'] == 1.0]\n",
    "merged_df_pe3EC = merged_df_pe3EC.sort_values(by='mean', ascending=False)\n",
    "merged_df_pe3EC = merged_df_pe3EC.dropna(subset=['mean'])\n",
    "merged_df_pe3EC = merged_df_pe3EC[merged_df_pe3EC['Module'].str.startswith('EC')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47431a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pe2 = find_mean_of_means_combinations(merged_df_pe2, target_units=2.0)\n",
    "result_pe3g = find_mean_of_means_combinations(merged_df_pe3GV, target_units= 1.0)\n",
    "result_pe3e = find_mean_of_means_combinations(merged_df_pe3EC, target_units=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ab676",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_dict_pe2 = {\n",
    "    'Hard': ['GV248','GV262','GV325','GV313','EC339','EC335','LL106','MA102','ST101A'],\n",
    "    'Medium': ['GV263','GV251','GV328','GV316','EC334','EC339','LL221','LL205'],\n",
    "    'Easy': ['GV249','GV245','GV314','EC338','EC334','DS105A','AC102','EH209']\n",
    "}\n",
    "year1_combos_dfpe3 =year1_combos_dfpe2.copy()\n",
    "year1_combos_dfpe3['Modules'] = year1_combos_dfpe3.apply(\n",
    "    lambda row: list(set(row['Compulsory Modules'] + append_dict_pe2.get(row['Difficulty'], []))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "year1_combos_dfpe3['Optional Modules Added'] = year1_combos_dfpe3['Difficulty'].apply(\n",
    "    lambda diff: append_dict_pe2.get(diff, [])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_combos_dfpe3[['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']] = year1_combos_dfpe3['Modules'].apply(\n",
    "    lambda x: pd.Series(\n",
    "        calculate_statistics_for_modules(x, summary_df)\n",
    "        if calculate_statistics_for_modules(x, summary_df) is not None\n",
    "        else [np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        index=['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']\n",
    "    )\n",
    ")\n",
    "year1_combos_dfpe3[\"degree\"] = \"BSc Politics and Economics\"\n",
    "year1_combos_dfpe3[[\"Optional Modules Added\",\"Difficulty\",\"Mean\",\"Median\",\"Q10\",\"Q90\",\"Pooled_SD\",\"degree\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c82bf1",
   "metadata": {},
   "source": [
    "**4.44.2 Final Table**\n",
    "* There does seem to be a little bit more of a range between module choices here, but once again nothing that is realistically too significant. Our hypothesis was wrong here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94776ba",
   "metadata": {},
   "source": [
    "### 4.45 Psychological and Behavioural Sciences\n",
    "\n",
    "\n",
    "**4.45.1 Hypothesis** \n",
    "* We predict that as PBS is a course which is easy, and has compulsory modules which average out to the highest mean, then potentially the difference between high and low is enough to take the score above 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_options = pd.read_csv(\"data/degrees/pbsm.csv\")\n",
    "pb_options.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_combinations_dfpb = summarize_compulsory_modules(pb_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_combos_dfpb = generate_year1_optional_combinations(initial_combinations_dfpb, pb_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3212e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_combos_dfpb2= create_difficulty_subrows(year1_combos_dfpb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_pb = add_summary_and_prereqs(pb_options, summary_df,outside_options_marks)\n",
    "merged_df_pb2= merged_df_pb[merged_df_pb['Year'] == 3]\n",
    "merged_df_pb2 = merged_df_pb2.sort_values(by='mean', ascending=False)\n",
    "merged_df_pb2 = merged_df_pb2.dropna(subset=['mean'])\n",
    "merged_df_pb2 = merged_df_pb2[merged_df_pb2['Optional/Compulsory'] == 1.0]\n",
    "result_pb2 = find_mean_of_means_combinations(merged_df_pb2, target_units=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b485b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_dict_pb3 = {\n",
    "    'Hard': ['AC103','EC1B3','EC1A3','PB307'],\n",
    "    'Medium': ['LL210','AN237','PB314'],\n",
    "    'Easy': ['DS105A','EH215','AC102','PB312']\n",
    "}\n",
    "\n",
    "year1_combos_dfpb3 =year1_combos_dfpb2.copy()\n",
    "year1_combos_dfpb3['Modules'] = year1_combos_dfpb3.apply(\n",
    "    lambda row: list(set(row['Modules'] + append_dict_pb3.get(row['Difficulty'], []))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "year1_combos_dfpb3['Optional Modules Added'] = year1_combos_dfpb3['Difficulty'].apply(\n",
    "    lambda diff: append_dict_pb3.get(diff, [])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86337a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1_combos_dfpb3[['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']] = year1_combos_dfpb3['Modules'].apply(\n",
    "    lambda x: pd.Series(\n",
    "        calculate_statistics_for_modules(x, summary_df)\n",
    "        if calculate_statistics_for_modules(x, summary_df) is not None\n",
    "        else [np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        index=['Mean', 'Q10', 'Median', 'Q90', 'Pooled_SD']\n",
    "    )\n",
    ")\n",
    "year1_combos_dfpb3[\"degree\"] = \"BSc Psychological and Behavioural Science\"\n",
    "year1_combos_dfpb3[[\"Optional Modules Added\",\"Difficulty\",\"Mean\",\"Median\",\"Q10\",\"Q90\",\"Pooled_SD\",\"degree\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bcbfb2",
   "metadata": {},
   "source": [
    "**4.45.2 Final Table** \n",
    "* Our hypothesis was wrong here, PBS is also a relatively uninteresting course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983e17c",
   "metadata": {},
   "source": [
    "### 4.5  Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06617c03",
   "metadata": {},
   "source": [
    "We add all the tables together, and then select only the hardest and easiest possible combination a student can choose for each degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd70cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Comparison = pd.concat([year1_combos_dfpb3,year1_combos_dfpe3,year1_combos_dfirf2,year_1incOO_df2,finance_combos], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84aab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Comparison = Final_Comparison.loc[[0, 5, 18, 20, 42, 50,54,59,63,65]] \n",
    "Final_Comparison[[\"Difficulty\",\"Mean\",\"Median\",\"Q10\",\"Q90\",\"Pooled_SD\",\"degree\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ed9f1",
   "metadata": {},
   "source": [
    "## 5) Module Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54bb6f7",
   "metadata": {},
   "source": [
    "**5.1**\n",
    "* We first want to visualise the discrepancy for hard and easy degree desigsns for the degrees we have chosen to analyse. We put this in reference to thresholds of 70, and 65.5 which are respectively the score required for a first and the overall degree average needed to get a first if you have obtained 8 or more firsts overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees =Final_Comparison[\"degree\"].unique()\n",
    "easy_means = Final_Comparison[Final_Comparison[\"Difficulty\"] == 'Easy'][\"Mean\"].values\n",
    "hard_means =  Final_Comparison[Final_Comparison[\"Difficulty\"] == 'Hard'][\"Mean\"].values\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, degree in enumerate(degrees):\n",
    "    ax.plot([i, i], [hard_means[i], easy_means[i]], color='black')  # Line between hard and easy\n",
    "    ax.scatter(i, easy_means[i], color='green', label='Easy' if i==0 else \"\", s=100)\n",
    "    ax.scatter(i, hard_means[i], color='red', label='Hard' if i==0 else \"\", s=100)\n",
    "ax.axhline(65.5, color='blue', linestyle='--', label='65.5 Threshold')\n",
    "ax.axhline(70.0, color='purple', linestyle='--', label='70.0 Threshold')\n",
    "ax.set_xticks(range(len(degrees)))\n",
    "ax.set_xticklabels(degrees, rotation=45, ha='right')\n",
    "ax.set_ylabel('Mean Score')\n",
    "ax.set_title('Easy vs Hard Mean Scores by Degree')\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5887a",
   "metadata": {},
   "source": [
    "**5.2**\n",
    "* We can see here, that for every course except Finance, allows a student to surpass the 65.5 threshold which is a substantial advantage to the student. But no course takes a student above the 70% threshold. There is quite a difference in the ranges between the courses, so we will conduct further analysis to understand what creates these differences.  \n",
    "\n",
    "* Interestingly, looking at the t-values for each of the course designs, we can see tha conducting a hypothesis test comparing the means between the easy and hard design produces a t value which is below 1, for every single course. There are no degrees of freedom that produce values that low whilst still having a low signifcance level. This indicates that the difference can be purely based on statistical differences of the consitutuent modules and hence there is no likely difference. I.e a student can't really 'cheat the system'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa977833",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_means = Final_Comparison[Final_Comparison['Difficulty'] == 'Easy'].groupby('degree')['Mean'].mean().reset_index()\n",
    "easy_means.rename(columns={'Mean': 'Easy_Mean'}, inplace=True)\n",
    "\n",
    "hard_means = Final_Comparison[Final_Comparison['Difficulty'] == 'Hard'].groupby('degree')['Mean'].mean().reset_index()\n",
    "hard_means.rename(columns={'Mean': 'Hard_Mean'}, inplace=True)\n",
    "\n",
    "range_df = pd.merge(easy_means, hard_means, on='degree')\n",
    "\n",
    "range_df['Range'] = range_df['Easy_Mean'] - range_df['Hard_Mean']\n",
    "\n",
    "range_df['Primary Department'] = ['Economics', 'Finance', 'Relations', 'Government', 'Science']\n",
    "range_df\n",
    "optional_units = programms_df_clean[[\"degree\",\"total_optional_units\",'median_salary','acceptance_perc']]\n",
    "variance_table = range_df.copy()\n",
    "var_df = pd.merge(variance_table, department_dist, left_on='Primary Department', right_on='Department', how='left')\n",
    "opt_df = pd.merge(var_df, optional_units, left_on='degree', right_on='degree', how='left')\n",
    "opt_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ad051",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', context='notebook')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "axes = axes.flatten()\n",
    "sns.scatterplot(data=opt_df, x='Department_SD', y='Range', ax=axes[0], color='teal', s=100, edgecolor='black')\n",
    "x = opt_df['Department_SD']\n",
    "y = opt_df['Range']\n",
    "coeffs = np.polyfit(x, y, 1)\n",
    "x_line = np.linspace(x.min(), x.max(), 100)\n",
    "y_line = coeffs[0] * x_line + coeffs[1]\n",
    "axes[0].plot(x_line, y_line, linestyle='--', color='black')\n",
    "\n",
    "axes[0].set_title('Range vs Department SD', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Department SD', fontsize=12)\n",
    "axes[0].set_ylabel('Range', fontsize=12)\n",
    "axes[0].set_ylim(0, 7)\n",
    "axes[0].tick_params(width=2)\n",
    "for spine in axes[0].spines.values():\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "sns.scatterplot(data=opt_df,x='total_optional_units',y='Range',ax=axes[1],color='darkorange',s=100, edgecolor='black')\n",
    "\n",
    "x = opt_df['total_optional_units']\n",
    "y = opt_df['Range']\n",
    "coeffs = np.polyfit(x, y, 1)\n",
    "x_line = np.linspace(x.min(), x.max(), 100)\n",
    "y_line = coeffs[0] * x_line + coeffs[1]\n",
    "axes[1].plot(x_line, y_line, linestyle='--', color='black')\n",
    "\n",
    "axes[1].set_title('Range vs Total Optional Units', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Total Optional Units', fontsize=12)\n",
    "axes[1].set_ylabel('Range', fontsize=12)\n",
    "axes[1].set_ylim(0, 7)\n",
    "axes[1].tick_params(width=2)\n",
    "for spine in axes[1].spines.values():\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf0f14",
   "metadata": {},
   "source": [
    "**5.3**\n",
    "* The first plot analyses how range could be affected by department sd, this is a metric which looks at the difference between the hard and easy modules within the primary department of the course. We looked at this for the reason that because this is a driver between the hard and easy course designs. Certain courses like economics have a lot of variance, and as expected they have a large range. But the overall pattern is unclear. There is an upwards trend but there are lot of courses concentrated around a specific Department SD which makes the trend less obvious. One issue of this is that some courses have options in different department modules, e.g finance which has no option in Finance. \n",
    "\n",
    "* The second plot analyses how the range is impacted by the number of optional unis within a degree. This pattern is even more clouded with the regression predicting no pattern. We suspect this is because even though a course may have a large degree of optionality, there are lot of other factors which come into play in explaining the range. One such example may be the number of options the student gets to choose from, (IR has a lot of units of optionality, but overall not many more options to choose from). In a more ideal setting there is a more comprenshive metric of optionality, but the presence of outside options skews the analysis quite significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dif= programms_df_clean[['degree','Mean']]\n",
    "diff_df = pd.merge(opt_df.copy(), baseline_dif, left_on='degree', right_on='degree', how='left')\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "x_vars = ['median_salary', 'acceptance_perc', 'Mean']\n",
    "titles = ['Easy Mean vs Median Salary', 'Easy Mean vs Acceptance %', 'Easy Mean vs Mean of Compulsory Modules']\n",
    "colors = ['royalblue', 'seagreen', 'darkred']\n",
    "\n",
    "for i, x_var in enumerate(x_vars):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    sns.scatterplot(data=diff_df,x=x_var,y='Easy_Mean',ax=ax,color=colors[i],s=100,edgecolor='black')\n",
    "    x = diff_df[x_var]\n",
    "    y = diff_df['Easy_Mean']\n",
    "    coeffs = np.polyfit(x, y, 1)\n",
    "    x_line = np.linspace(x.min(), x.max(), 100)\n",
    "    y_line = coeffs[0] * x_line + coeffs[1]\n",
    "    ax.plot(x_line, y_line, linestyle='--', color='black')\n",
    "    ax.set_title(titles[i], fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(x_var.replace('_', ' ').title(), fontsize=12)\n",
    "    ax.set_ylabel('Easy_Mean', fontsize=12)\n",
    "    ax.tick_params(width=2)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30088aaa",
   "metadata": {},
   "source": [
    "**5.4**\n",
    "\n",
    "* Range between hard and easy is something that is impacted by some other factors, but we also want to analyse how difficult a course is, based on certain factors and how this correlates with outcomes. I.e can you pick an easy degree and unlock good enough outcomes. \n",
    "\n",
    "* The first plot shows that as a course gets harder, the median salary by graduates (after 15 months) gets higher. This is relatively in line with what is expected. With there being so few data points the actual relationship is hard to confirm. As Economics has a relatively easy design and a high grad salary so we will not make an asssertive statement here.\n",
    "* The second plot shows that acceptance rate has no relationship with how easy a course is. This is possibly because there are so many other factors driving acceptance %, and prospective students do not have an idea of how easy a course is until they get to LSE. The factor we would attribute to this would be 'prestige'. This 'prestige' is a factor in our opinion driven by acceptance rate, grad salary and how difficult a course is. But this is a very hard factor to isolate and measure\n",
    "* The third plot is a definitive relationship that as the compulsory courses get easier, the highest mean outcome for the easy design increases. This is an expected driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de73b43",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e5a9e",
   "metadata": {},
   "source": [
    "#### Summary of Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5776b92b",
   "metadata": {},
   "source": [
    "* Based on the analysis we have done above, we do not believe it is possible to 'game' the system and choose easy modules to increase your overall degree score. This is due to statistical variation in the course results, assuming that the student is as good as the average. Even despite the statistical variation, the largest range is only 6% and is only ever enough to push a student above the 1st threshold to obtain a first. Additionally, this range is very dependent on the % of courses in specific departments as some departments have a large range of hard and easy courses. In terms of graduate outcome, it is unclear whether an easy degree really correlates with higher grad salary and the difficulty of joining a course has little to do with how easy a course is. Overall selecting a 'percieved easy degree to get into' does not mean that it is an easy course and hence a degree with higher salary. Overall LSE has a relatively robust degree system that is well designed and fair.  \n",
    "\n",
    "* Some courses have given recommendations which are actually the easiest possible combination that a student can take, such as BSc Finance. The mean outcome is always in the 2:1 classification, q10 is never a fail and q90 is always a first, regardless of difficulty. The hardest modules at LSE are Maths and Economics. We have also found that the A- Level requirement for a course is actually well correlated with graduate salary. Most degrees have graduate salary between a thin band apart from a few outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9adf3bb",
   "metadata": {},
   "source": [
    "#### Limitiations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abeecf2",
   "metadata": {},
   "source": [
    "As discussed, there are quite a lot of limitations in our analaysis. A lot of these have mentioned beforehand but we will run through a quick summary of the limitations and how they impacted the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b0cc8",
   "metadata": {},
   "source": [
    "* **Only Choosing 5 Degrees** - This was done due to the difficulty of completing an accurate and comprehensive study of the degrees at LSE. We have chose different specific charateristics here which would be a reason for having a large difference between hard and easy.\n",
    "* **Missing Courses** - Some courses have no data as they didnt exist in the last year. Therefore the calcualtion for mean has simply skipped it over\n",
    "* **Students may have correlated performance** - we have made an assumption in our t testing, that the student is average at every module, but if they are good at some courses they are likely to be good at others which impacts the standard error for each course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eac576",
   "metadata": {},
   "source": [
    "#### What else could be done?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b045d9",
   "metadata": {},
   "source": [
    "The natural next step would be to analyse the rest of the degrees at LSE, and adapting our code to be able to process course combinations fully automatically, this would involve creating a significantly more intelligent scraping bot. With all of this we can comprenhesively compare different charateristics to make a more informed decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677720fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6660fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
